{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ebdb378",
   "metadata": {},
   "source": [
    "# 推理JIT优化\n",
    "本节介绍如何利用 MindSpore 的 JIT（Just-In-Time）编译技术，对 `DeepSeek-R1-Distill-Qwen-1.5B` 模型进行推理优化。通过开启 JIT 编译，降低单次推理耗时，从而提升对话响应速度与用户体验。\n",
    "优化实践可参考示例代码：[deepseek-r1-distill-qwen-1.5b-jit.py](https://github.com/mindspore-courses/orange-pi-mindspore/blob/master/Online/training/01-DeepSeek-R1-Distill-Qwen-1.5B/deepseek-r1-distill-qwen-1.5b-jit.py)\n",
    "\n",
    ">本教程仅适用于 昇思大模型平台的单卡环境，在昇腾开发板上的实际操作，请以上述示例代码为准。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "623773ca-3030-4297-8917-7ebf05f5b60e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture captured_output\n",
    "# 实验环境已经预装了mindspore==2.6.0，如需更换mindspore版本，可更改下面 MINDSPORE_VERSION 变量\n",
    "!pip uninstall mindspore -y\n",
    "%env MINDSPORE_VERSION=2.6.0\n",
    "!pip install https://ms-release.obs.cn-north-4.myhuaweicloud.com/${MINDSPORE_VERSION}/MindSpore/unified/aarch64/mindspore-${MINDSPORE_VERSION}-cp39-cp39-linux_aarch64.whl --trusted-host ms-release.obs.cn-north-4.myhuaweicloud.com -i https://pypi.tuna.tsinghua.edu.cn/simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d22a47a-1213-4383-8e34-7cd72e3da2f9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: mindspore\n",
      "Version: 2.6.0\n",
      "Summary: MindSpore is a new open source deep learning training/inference framework that could be used for mobile, edge and cloud scenarios.\n",
      "Home-page: https://www.mindspore.cn\n",
      "Author: The MindSpore Authors\n",
      "Author-email: contact@mindspore.cn\n",
      "License: Apache 2.0\n",
      "Location: /home/mindspore/miniconda/envs/jupyter/lib/python3.9/site-packages\n",
      "Requires: asttokens, astunparse, dill, numpy, packaging, pillow, protobuf, psutil, safetensors, scipy\n",
      "Required-by: mindnlp\n"
     ]
    }
   ],
   "source": [
    "# 查看当前 mindspore 版本\n",
    "!pip show mindspore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2e26bf3-4811-4dc9-8bf8-bd6345216034",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture captured_output\n",
    "# 安装mindnlp 0.4.1 版本\n",
    "!pip uninstall mindnlp -y\n",
    "!pip install https://xihe.mindspore.cn/coderepo/web/v1/file/MindSpore/mindnlp/main/media/mindnlp-0.4.1-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91def9dd-af74-4b7b-b142-ddd1be1f4f98",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mindspore/miniconda/envs/jupyter/lib/python3.9/site-packages/numpy/core/getlimits.py:549: UserWarning: The value of the smallest subnormal for <class 'numpy.float64'> type is zero.\n",
      "  setattr(self, word, getattr(machar, word).flat[0])\n",
      "/home/mindspore/miniconda/envs/jupyter/lib/python3.9/site-packages/numpy/core/getlimits.py:89: UserWarning: The value of the smallest subnormal for <class 'numpy.float64'> type is zero.\n",
      "  return self._float_to_str(self.smallest_subnormal)\n",
      "/home/mindspore/miniconda/envs/jupyter/lib/python3.9/site-packages/numpy/core/getlimits.py:549: UserWarning: The value of the smallest subnormal for <class 'numpy.float32'> type is zero.\n",
      "  setattr(self, word, getattr(machar, word).flat[0])\n",
      "/home/mindspore/miniconda/envs/jupyter/lib/python3.9/site-packages/numpy/core/getlimits.py:89: UserWarning: The value of the smallest subnormal for <class 'numpy.float32'> type is zero.\n",
      "  return self._float_to_str(self.smallest_subnormal)\n"
     ]
    }
   ],
   "source": [
    "import mindspore\n",
    "from mindnlp.transformers import AutoTokenizer, AutoModelForCausalLM, StaticCache\n",
    "from mindnlp.core import ops\n",
    "from mindnlp.configs import set_pyboost\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "# 开启O2级别的jit优化，开启图算融合\n",
    "mindspore.set_context(\n",
    "    enable_graph_kernel=True,\n",
    "    mode=mindspore.GRAPH_MODE,\n",
    "    jit_config={\n",
    "        \"jit_level\": \"O2\",\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92764274-55f3-45a4-8159-e2447196070f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sample_top_p(probs, p=0.9):\n",
    "    \"\"\"\n",
    "    Top-p采样函数，用于生成文本时选择下一个token。\n",
    "    此处优先采用基于numpy而不是原生MindSpore的实现方式，因为在香橙派上运行效率更高\n",
    "    \"\"\"\n",
    "    probs_np = probs.asnumpy()\n",
    "    # 按概率降序排序\n",
    "    sorted_indices = np.argsort(-probs_np, axis=-1)\n",
    "    sorted_probs = np.take_along_axis(probs_np, sorted_indices, axis=-1)\n",
    "    # 计算累积概率并创建掩码\n",
    "    cumulative_probs = np.cumsum(sorted_probs, axis=-1)\n",
    "    mask = cumulative_probs - sorted_probs > p\n",
    "    sorted_probs[mask] = 0.0\n",
    "    sorted_probs = sorted_probs / np.sum(sorted_probs, axis=-1, keepdims=True)\n",
    "    # 转换回MindSpore Tensor\n",
    "    sorted_probs_tensor = mindspore.Tensor(sorted_probs, dtype=mindspore.float32)\n",
    "    sorted_indices_tensor = mindspore.Tensor(sorted_indices, dtype=mindspore.int32)\n",
    "    next_token_idx = ops.multinomial(sorted_probs_tensor, 1)\n",
    "    batch_size = probs.shape[0]\n",
    "    batch_indices = ops.arange(0, batch_size, dtype=mindspore.int32).reshape(-1, 1)\n",
    "    # 此处采用基于mindspore.ops的实现方式，在香橙派上兼容性最好\n",
    "    # next_token = sorted_indices_tensor[batch_indices, next_token_idx]\n",
    "    next_token = mindspore.ops.gather(sorted_indices_tensor, next_token_idx, axis=1, batch_dims=1)\n",
    "    # next_token = mindspore.mint.gather(sorted_indices_tensor, dim=1, index=next_token_idx)\n",
    "    return next_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fce02389-4095-4f7b-92d5-a15b6489c4cc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Qwen2ForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`.`PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "Sliding Window Attention is enabled but not implemented for `eager`; unexpected results may be encountered.\n"
     ]
    }
   ],
   "source": [
    "# 该任务将使用DeepSeek-R1-Distill-Qwen-1.5B模型，对给定的prompt进行补齐\n",
    "prompts = [\n",
    "    \"请介绍一下自己。<think>\",\n",
    "    \"My favorite all time favorite condiment is ketchup.\",\n",
    "]\n",
    "\n",
    "# 生成参数配置\n",
    "NUM_TOKENS_TO_GENERATE = 40  # 每个输入要生成的token数量\n",
    "TEMPERATURE = 0.8            # 温度参数（控制生成多样性）\n",
    "TOP_P = 0.8                  # Top-p采样阈值\n",
    "\n",
    "model_id = \"MindSpore-Lab/DeepSeek-R1-Distill-Qwen-1.5B-FP16\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, mirror=\"modelers\")\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, low_cpu_mem_usage=True, mirror=\"modelers\")\n",
    "\n",
    "# 使用model.jit()将全图静态图化\n",
    "model.jit()\n",
    "\n",
    "inputs = tokenizer(prompts, return_tensors=\"ms\", padding=True)\n",
    "set_pyboost(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "037bfa61-f008-4347-ac15-824fc1ea3165",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 使用@mindspore.jit装饰器封装模型推理函数\n",
    "@mindspore.jit(jit_config=mindspore.JitConfig(jit_syntax_level='STRICT'))\n",
    "def get_decode_one_tokens_logits(model, cur_token, input_pos, cache_position, past_key_values, temperature=TEMPERATURE, top_p=TOP_P):\n",
    "    \"\"\"单个token的解码函数，返回logits，可以使用jit进行优化\"\"\"\n",
    "    logits = model(\n",
    "        cur_token,\n",
    "        position_ids=input_pos,\n",
    "        cache_position=cache_position,\n",
    "        past_key_values=past_key_values,\n",
    "        return_dict=False,\n",
    "        use_cache=True\n",
    "    )[0]\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53aec62b-a005-4381-991e-766e9a0faf02",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ERROR] CORE(1765,ffffbdb5c020,python3.9):2025-07-11-01:46:21.129.015 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_1765/2065961418.py]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".[1]: 18.387741088867188\n",
      "[2]: 0.11385655403137207\n",
      "[3]: 0.1119377613067627\n",
      "[4]: 0.11188721656799316\n",
      "[5]: 0.11326718330383301\n",
      "[6]: 0.1119697093963623\n",
      "[7]: 0.11219620704650879\n",
      "[8]: 0.11276006698608398\n",
      "[9]: 0.11196351051330566\n",
      "[10]: 0.11230587959289551\n",
      "[11]: 0.11212468147277832\n",
      "[12]: 0.11254668235778809\n",
      "[13]: 0.11221146583557129\n",
      "[14]: 0.1121068000793457\n",
      "[15]: 0.1147160530090332\n",
      "[16]: 0.11426210403442383\n",
      "[17]: 0.11398482322692871\n",
      "[18]: 0.11723160743713379\n",
      "[19]: 0.11890697479248047\n",
      "[20]: 0.1141505241394043\n",
      "[21]: 0.11414146423339844\n",
      "[22]: 0.11372566223144531\n",
      "[23]: 0.11393380165100098\n",
      "[24]: 0.11448025703430176\n",
      "[25]: 0.115509033203125\n",
      "[26]: 0.12166905403137207\n",
      "[27]: 0.12461304664611816\n",
      "[28]: 0.11643409729003906\n",
      "[29]: 0.11528921127319336\n",
      "[30]: 0.11597490310668945\n",
      "[31]: 0.11604595184326172\n",
      "[32]: 0.11636590957641602\n",
      "[33]: 0.11526799201965332\n",
      "[34]: 0.11614346504211426\n",
      "[35]: 0.11723947525024414\n",
      "[36]: 0.11544680595397949\n",
      "[37]: 0.11590099334716797\n",
      "[38]: 0.11560893058776855\n",
      "[39]: 0.11474037170410156\n",
      "['请介绍一下自己。<think>\\n嗯，用户让我介绍一下自己。首先，我得考虑他们为什么会问这个问题。可能是在正式场合，比如准备演讲或者做介绍视频，或者是想了解自己的背景。不管怎样，!', 'My favorite all time favorite condiment is ketchup. I have a recipe that makes a large batch of ketchup. The recipe requires 160 grams of ketchup. I need to make enough ketchup for my large batch recipe. But I!']\n"
     ]
    }
   ],
   "source": [
    "def decode_one_tokens(model, cur_token, input_pos, cache_position, past_key_values, temperature=TEMPERATURE, top_p=TOP_P):\n",
    "    \"\"\"单个token的解码函数，由logits、温度和Top_p选择合适的token\"\"\"\n",
    "    logits = get_decode_one_tokens_logits(model, cur_token, input_pos, cache_position, past_key_values, temperature, top_p)\n",
    "\n",
    "    if temperature > 0:\n",
    "        probs = mindspore.mint.softmax(logits[:, -1] / temperature, dim=-1)\n",
    "        new_token = sample_top_p(probs, top_p)\n",
    "    else:\n",
    "        new_token = mindspore.mint.argmax(logits[:, -1], dim=-1)[:, None]\n",
    "\n",
    "    return new_token\n",
    "\n",
    "\n",
    "batch_size, seq_length = inputs[\"input_ids\"].shape\n",
    "\n",
    "# 创建静态缓存（用于加速自回归生成）\n",
    "past_key_values = StaticCache(\n",
    "    config=model.config, max_batch_size=2, max_cache_len=512, dtype=model.dtype\n",
    ")\n",
    "cache_position = ops.arange(seq_length)\n",
    "generated_ids = ops.zeros(\n",
    "    batch_size, seq_length + NUM_TOKENS_TO_GENERATE + 1, dtype=mindspore.int32\n",
    ")\n",
    "generated_ids[:, cache_position] = inputs[\"input_ids\"].to(mindspore.int32)\n",
    "\n",
    "# 初始前向传播获取首个logits\n",
    "logits = model(\n",
    "    **inputs, cache_position=cache_position, past_key_values=past_key_values,return_dict=False, use_cache=True\n",
    ")[0]\n",
    "\n",
    "# 生成第一个新token\n",
    "if TEMPERATURE > 0:\n",
    "    probs = mindspore.mint.softmax(logits[:, -1] / TEMPERATURE, dim=-1)\n",
    "    next_token = sample_top_p(probs, TOP_P)\n",
    "else:\n",
    "    next_token = mindspore.mint.argmax(logits[:, -1], dim=-1)[:, None]\n",
    "\n",
    "generated_ids[:, seq_length] = next_token[:, 0]\n",
    "\n",
    "# 自回归生成循环\n",
    "cache_position = mindspore.tensor([seq_length + 1])\n",
    "for i in range(1, NUM_TOKENS_TO_GENERATE):\n",
    "    s = time.time()\n",
    "    next_token = decode_one_tokens(model, next_token, None, cache_position, past_key_values)\n",
    "    generated_ids[:, cache_position] = next_token.int()\n",
    "    cache_position += 1\n",
    "    t = time.time()\n",
    "    # 打印单步生成耗时\n",
    "    print(\"[%d]:\" % i, t - s)\n",
    "\n",
    "text = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "print(text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
