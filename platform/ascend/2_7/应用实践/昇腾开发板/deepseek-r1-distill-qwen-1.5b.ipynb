{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd8d8037",
   "metadata": {},
   "source": [
    "# 模型推理\n",
    "本章节将介绍如何对 `DeepSeek-R1-Distill-Qwen-1.5B` 模型进行推理部署，并构建一个可交互的对话机器人，以提升模型的应用性与用户体验。\n",
    "\n",
    "推理示例代码参考：[deepseek-r1-distill-qwen-1.5b-gradio.py](https://github.com/mindspore-courses/orange-pi-mindspore/blob/master/Online/training/01-DeepSeek-R1-Distill-Qwen-1.5B/deepseek-r1-distill-qwen-1.5b-gradio.py)\n",
    "\n",
    "\n",
    ">本教程仅适用于 昇思大模型平台的单卡环境，在昇腾开发板上的实际操作，请以上述示例代码为准。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5d9096e-e24a-4380-8fee-03590a3755c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture captured_output\n",
    "# 实验环境已经预装了mindspore==2.6.0，如需更换mindspore版本，可更改下面 MINDSPORE_VERSION 变量\n",
    "!pip uninstall mindspore -y\n",
    "%env MINDSPORE_VERSION=2.6.0\n",
    "!pip install https://ms-release.obs.cn-north-4.myhuaweicloud.com/${MINDSPORE_VERSION}/MindSpore/unified/aarch64/mindspore-${MINDSPORE_VERSION}-cp39-cp39-linux_aarch64.whl --trusted-host ms-release.obs.cn-north-4.myhuaweicloud.com -i https://pypi.tuna.tsinghua.edu.cn/simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c5597db-6476-4405-9ca7-6cd18a6cdf96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: mindspore\n",
      "Version: 2.6.0\n",
      "Summary: MindSpore is a new open source deep learning training/inference framework that could be used for mobile, edge and cloud scenarios.\n",
      "Home-page: https://www.mindspore.cn\n",
      "Author: The MindSpore Authors\n",
      "Author-email: contact@mindspore.cn\n",
      "License: Apache 2.0\n",
      "Location: /home/mindspore/miniconda/envs/jupyter/lib/python3.9/site-packages\n",
      "Requires: asttokens, astunparse, dill, numpy, packaging, pillow, protobuf, psutil, safetensors, scipy\n",
      "Required-by: mindnlp\n"
     ]
    }
   ],
   "source": [
    "# 查看当前 mindspore 版本\n",
    "!pip show mindspore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "801d3ff6-f004-47ac-a5e4-4aa5114fd7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture captured_output\n",
    "# 安装mindnlp 0.4.1 版本\n",
    "!pip uninstall mindnlp -y\n",
    "!pip install https://xihe.mindspore.cn/coderepo/web/v1/file/MindSpore/mindnlp/main/media/mindnlp-0.4.1-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d59b0c3-0dd3-4b1b-8590-c6e222f1f4f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mindspore/miniconda/envs/jupyter/lib/python3.9/site-packages/numpy/core/getlimits.py:549: UserWarning: The value of the smallest subnormal for <class 'numpy.float64'> type is zero.\n",
      "  setattr(self, word, getattr(machar, word).flat[0])\n",
      "/home/mindspore/miniconda/envs/jupyter/lib/python3.9/site-packages/numpy/core/getlimits.py:89: UserWarning: The value of the smallest subnormal for <class 'numpy.float64'> type is zero.\n",
      "  return self._float_to_str(self.smallest_subnormal)\n",
      "/home/mindspore/miniconda/envs/jupyter/lib/python3.9/site-packages/numpy/core/getlimits.py:549: UserWarning: The value of the smallest subnormal for <class 'numpy.float32'> type is zero.\n",
      "  setattr(self, word, getattr(machar, word).flat[0])\n",
      "/home/mindspore/miniconda/envs/jupyter/lib/python3.9/site-packages/numpy/core/getlimits.py:89: UserWarning: The value of the smallest subnormal for <class 'numpy.float32'> type is zero.\n",
      "  return self._float_to_str(self.smallest_subnormal)\n",
      "Qwen2ForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`.`PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "Sliding Window Attention is enabled but not implemented for `eager`; unexpected results may be encountered.\n"
     ]
    }
   ],
   "source": [
    "from mindnlp.transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from mindnlp.transformers import TextIteratorStreamer\n",
    "from mindnlp.peft import PeftModel\n",
    "from threading import Thread\n",
    "\n",
    "# 开启同步，在出现报错，定位问题时开启\n",
    "# mindspore.set_context(pynative_synchronize=True)\n",
    "\n",
    "# Loading the tokenizer and model from Modelers's model hub.\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"MindSpore-Lab/DeepSeek-R1-Distill-Qwen-1.5B-FP16\", mirror=\"modelers\")\n",
    "# 设置pad_token为eos_token\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "model = AutoModelForCausalLM.from_pretrained(\"MindSpore-Lab/DeepSeek-R1-Distill-Qwen-1.5B-FP16\", mirror=\"modelers\")\n",
    "# adapter_model path\n",
    "# model = PeftModel.from_pretrained(model, \"./output/DeepSeek-R1-Distill-Qwen-1.5B/adapter_model_for_demo/\")\n",
    "\n",
    "system_prompt = \"你是一个智能聊天机器人，以最简单的方式回答用户问题\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "549f37ca-9718-4957-9b7c-37000bc6a970",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_input_from_chat_history(chat_history, msg: str):\n",
    "    messages = [{'role': 'system', 'content': system_prompt}]\n",
    "    for info in chat_history:\n",
    "        role, content = info['role'], info['content']\n",
    "        messages.append({'role': role, 'content': content})\n",
    "    messages.append({'role': 'user', 'content': msg})\n",
    "    return messages\n",
    "\n",
    "\n",
    "def inference(message, history):\n",
    "    messages = build_input_from_chat_history(history, message)\n",
    "    input_ids = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            add_generation_prompt=True,\n",
    "            return_tensors=\"ms\",\n",
    "            tokenize=True\n",
    "        )\n",
    "\n",
    "    streamer = TextIteratorStreamer(tokenizer, timeout=300, skip_prompt=True, skip_special_tokens=True)\n",
    "    generate_kwargs = dict(\n",
    "        input_ids=input_ids,\n",
    "        streamer=streamer,\n",
    "        max_new_tokens=1024,\n",
    "        use_cache=True,\n",
    "    )\n",
    "\n",
    "    t = Thread(target=model.generate, kwargs=generate_kwargs)\n",
    "    t.start()  # Starting the generation in a separate thread.\n",
    "    partial_message = \"\"\n",
    "    for new_token in streamer:\n",
    "        partial_message += new_token\n",
    "        print(new_token, end=\"\", flush=True)\n",
    "\n",
    "    messages.append({'role': 'assistant', 'content': partial_message})\n",
    "    return messages[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f539a75-7513-4d73-9842-fe448ea028d2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "欢迎使用 DeepSeek-R1-Distill-Qwen-1.5B 模型，输入内容即可进行对话，clear 清空对话历史，stop 终止程序\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "用户： 你好\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DeepSeek-R1-Distill-Qwen-1.5B：您好！我是由深度求索公司独立开发的智能助手DeepSeek-R1，很高兴为您提供服务！请问您现在想要了解关于哪些方面？\n",
      "</think>\n",
      "\n",
      "你好！我是由深度求索公司独立开发的智能助手DeepSeek-R1，很高兴为您提供服务！请问您现在想要了解关于哪些方面？\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "用户： stop\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import platform\n",
    "\n",
    "\n",
    "os_name = platform.system()\n",
    "clear_command = 'cls' if os_name == 'Windows' else 'clear'\n",
    "welcome_prompt = '欢迎使用 DeepSeek-R1-Distill-Qwen-1.5B 模型，输入内容即可进行对话，clear 清空对话历史，stop 终止程序'\n",
    "print(welcome_prompt)\n",
    "history = []\n",
    "while True:\n",
    "    query = input(\"\\n用户：\")\n",
    "    if query.strip() == \"stop\":\n",
    "        break\n",
    "    if query.strip() == \"clear\":\n",
    "        os.system(clear_command)\n",
    "        print(welcome_prompt)\n",
    "        continue\n",
    "    print(\"\\nDeepSeek-R1-Distill-Qwen-1.5B：\", end=\"\")\n",
    "    history = inference(query, history)\n",
    "    print(\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
