{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49f06dfc",
   "metadata": {},
   "source": [
    "# 模型微调\n",
    "目前，[MindSpore NLP 仓库 0.4 分支](https://github.com/mindspore-lab/mindnlp/tree/0.4) 已提供适配昇腾开发板的 Qwen 模型支持。\n",
    "\n",
    "本章节将介绍如何基于昇思大模型平台，对 `DeepSeek-R1-Distill-Qwen-1.5B` 模型进行 LoRA 微调，使得模型可以模仿《甄嬛传》中甄嬛的语气风格进行对话。\n",
    "\n",
    "微调示例代码可参考：[deepseek-r1-distill-qwen-1.5b-lora.py](https://github.com/mindspore-courses/orange-pi-mindspore/blob/master/Online/training/01-DeepSeek-R1-Distill-Qwen-1.5B/deepseek-r1-distill-qwen-1.5b-lora.py)\n",
    "\n",
    ">本教程仅适用于 昇思大模型平台的单卡环境，在昇腾开发板上的实际操作，请以上述示例代码为准。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b4ac75",
   "metadata": {},
   "source": [
    "## LoRA介绍\n",
    "LoRA（Low-Rank Adaptation）是一种高效的参数微调方法，属于参数高效微调（Parameter-Efficient Fine-Tuning, PEFT）范式。其核心思想是在冻结原始模型参数的基础上，在 Attention 层中的 Query、Key、Value（QKV）等模块引入一个低秩旁路结构。\n",
    "\n",
    "该旁路由两个可训练的低维矩阵 A 和 B 组成，替代对原始大矩阵的直接更新。微调过程中仅更新 A 和 B，从而显著减少训练参数量，降低计算与内存开销，同时保持接近全参数微调的性能表现。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32c7bc6",
   "metadata": {},
   "source": [
    "### 环境准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b1edda1-d3a7-4237-b860-d1ffc4c390d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture captured_output\n",
    "# 实验环境已经预装了mindspore==2.6.0，如需更换mindspore版本，可更改下面 MINDSPORE_VERSION 变量\n",
    "!pip uninstall mindspore -y\n",
    "%env MINDSPORE_VERSION=2.6.0\n",
    "!pip install https://ms-release.obs.cn-north-4.myhuaweicloud.com/${MINDSPORE_VERSION}/MindSpore/unified/aarch64/mindspore-${MINDSPORE_VERSION}-cp39-cp39-linux_aarch64.whl --trusted-host ms-release.obs.cn-north-4.myhuaweicloud.com -i https://pypi.tuna.tsinghua.edu.cn/simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "139f1d89-9c39-47e8-8b5f-a55b5eaea2b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: mindspore\n",
      "Version: 2.6.0\n",
      "Summary: MindSpore is a new open source deep learning training/inference framework that could be used for mobile, edge and cloud scenarios.\n",
      "Home-page: https://www.mindspore.cn\n",
      "Author: The MindSpore Authors\n",
      "Author-email: contact@mindspore.cn\n",
      "License: Apache 2.0\n",
      "Location: /home/mindspore/miniconda/envs/jupyter/lib/python3.9/site-packages\n",
      "Requires: asttokens, astunparse, dill, numpy, packaging, pillow, protobuf, psutil, safetensors, scipy\n",
      "Required-by: mindnlp\n"
     ]
    }
   ],
   "source": [
    "# 查看当前 mindspore 版本\n",
    "!pip show mindspore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b9c5d45-92a0-4677-83f8-4278e7db0156",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture captured_output\n",
    "# 安装mindnlp 0.4.1 版本\n",
    "!pip uninstall mindnlp -y\n",
    "!pip install https://xihe.mindspore.cn/coderepo/web/v1/file/MindSpore/mindnlp/main/media/mindnlp-0.4.1-py3-none-any.whl\n",
    "# 安装openMind Hub Client\n",
    "!pip install openmind_hub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5604715",
   "metadata": {},
   "source": [
    "### 下载与处理数据集\n",
    "通过openmind_hub提供的接口下载 huanhuan.json 数据集："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e2bfb64-3c46-44e6-8af6-24001bc38368",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mindspore/miniconda/envs/jupyter/lib/python3.9/site-packages/numpy/core/getlimits.py:549: UserWarning: The value of the smallest subnormal for <class 'numpy.float64'> type is zero.\n",
      "  setattr(self, word, getattr(machar, word).flat[0])\n",
      "/home/mindspore/miniconda/envs/jupyter/lib/python3.9/site-packages/numpy/core/getlimits.py:89: UserWarning: The value of the smallest subnormal for <class 'numpy.float64'> type is zero.\n",
      "  return self._float_to_str(self.smallest_subnormal)\n",
      "/home/mindspore/miniconda/envs/jupyter/lib/python3.9/site-packages/numpy/core/getlimits.py:549: UserWarning: The value of the smallest subnormal for <class 'numpy.float32'> type is zero.\n",
      "  setattr(self, word, getattr(machar, word).flat[0])\n",
      "/home/mindspore/miniconda/envs/jupyter/lib/python3.9/site-packages/numpy/core/getlimits.py:89: UserWarning: The value of the smallest subnormal for <class 'numpy.float32'> type is zero.\n",
      "  return self._float_to_str(self.smallest_subnormal)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62f6ea2d67c04459bfbd5698a034d141",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from mindnlp.transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from mindnlp.engine import TrainingArguments, Trainer\n",
    "from mindnlp.dataset import load_dataset\n",
    "from mindnlp.transformers import GenerationConfig\n",
    "from mindnlp.peft import LoraConfig, TaskType, get_peft_model\n",
    "\n",
    "from mindnlp.engine.utils import PREFIX_CHECKPOINT_DIR\n",
    "from mindnlp.configs import SAFE_WEIGHTS_NAME\n",
    "from mindnlp.engine.callbacks import TrainerCallback, TrainerState, TrainerControl\n",
    "from mindspore._c_expression import disable_multi_thread\n",
    "\n",
    "disable_multi_thread()  # 禁用多线程，提升微调性能\n",
    "\n",
    "# 开启同步，在出现报错，定位问题时开启\n",
    "# mindspore.set_context(pynative_synchronize=True)\n",
    "\n",
    "from openmind_hub import om_hub_download\n",
    "\n",
    "\n",
    "# 从魔乐社区下载数据集\n",
    "om_hub_download(\n",
    "    repo_id=\"MindSpore-Lab/huanhuan\",\n",
    "    repo_type=\"dataset\",\n",
    "    filename=\"huanhuan.json\",\n",
    "    local_dir=\"./\",\n",
    ")\n",
    "\n",
    "# 加载数据集\n",
    "dataset = load_dataset(path=\"json\", data_files=\"./huanhuan.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8dd9cbe5-a41f-46fc-a7e0-e6dcf6bf7433",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 实例化tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"MindSpore-Lab/DeepSeek-R1-Distill-Qwen-1.5B-FP16\", mirror=\"modelers\", use_fast=False)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = 'right'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "556393dd-236f-43bf-9b38-e8641b45c399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: 小姐，别的秀女都在求中选，唯有咱们小姐想被撂牌子，菩萨一定记得真真儿的——\n",
      "\n",
      "Assistant: 嘘——都说许愿说破是不灵的。<｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜>\n"
     ]
    }
   ],
   "source": [
    "# 定义数据处理逻辑\n",
    "def process_func(instruction, input, output):\n",
    "    MAX_SEQ_LENGTH = 64  # 最长序列长度\n",
    "    input_ids, attention_mask, labels = [], [], []\n",
    "    # 首先生成user和assistant的对话模板\n",
    "    # User: instruction + input\n",
    "    # Assistant: output\n",
    "    formatted_instruction = tokenizer(f\"User: {instruction}{input}\\n\\n\", add_special_tokens=False)\n",
    "    formatted_response = tokenizer(f\"Assistant: {output}\", add_special_tokens=False)\n",
    "    # 最后添加 eos token，在deepseek-r1-distill-qwen的词表中， eos_token 和 pad_token 对应同一个token\n",
    "    # User: instruction + input \\n\\n Assistant: output + eos_token\n",
    "    input_ids = formatted_instruction[\"input_ids\"] + formatted_response[\"input_ids\"] + [tokenizer.pad_token_id]\n",
    "    # 注意，我们在微调时仅考虑Assistant部分回答的内容，所以User部分提问的内容对应的标签为-100\n",
    "    attention_mask = formatted_instruction[\"attention_mask\"] + formatted_response[\"attention_mask\"] + [1]\n",
    "    labels = [-100] * len(formatted_instruction[\"input_ids\"]) + formatted_response[\"input_ids\"] + [tokenizer.pad_token_id]\n",
    "\n",
    "    # 如超过最大长度，则进行截断\n",
    "    if len(input_ids) > MAX_SEQ_LENGTH:\n",
    "        input_ids = input_ids[:MAX_SEQ_LENGTH]\n",
    "        attention_mask = attention_mask[:MAX_SEQ_LENGTH]\n",
    "        labels = labels[:MAX_SEQ_LENGTH]\n",
    "\n",
    "    # 如不足最大长度，则进行填充\n",
    "    padding_length = MAX_SEQ_LENGTH - len(input_ids)\n",
    "    input_ids = input_ids + [tokenizer.pad_token_id] * padding_length\n",
    "    attention_mask = attention_mask + [0] * padding_length  # 填充的 attention_mask 为 0\n",
    "    labels = labels + [-100] * padding_length  # 填充的 label 为 -100\n",
    "\n",
    "    return input_ids, attention_mask, labels\n",
    "\n",
    "\n",
    "formatted_dataset = dataset.map(operations=[process_func], \n",
    "                                input_columns=['instruction', 'input', 'output'], \n",
    "                                output_columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "# 查看预处理后的数据\n",
    "for input_ids, attention_mask, labels in formatted_dataset.create_tuple_iterator():\n",
    "    print(tokenizer.decode(input_ids))\n",
    "    break\n",
    "\n",
    "# 为节约演示时间，将数据集裁剪\n",
    "truncated_dataset = formatted_dataset.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2befd865",
   "metadata": {},
   "source": [
    "### 执行微调"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b454527c-3afe-420b-a211-0e1dfd373914",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Qwen2ForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`.`PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "Sliding Window Attention is enabled but not implemented for `eager`; unexpected results may be encountered.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 9,232,384 || all params: 1,786,320,384 || trainable%: 0.5168380813819342\n"
     ]
    }
   ],
   "source": [
    "# 实例化base model\n",
    "model_id = \"MindSpore-Lab/DeepSeek-R1-Distill-Qwen-1.5B-FP16\"\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(model_id, mirror=\"modelers\")\n",
    "base_model.generation_config = GenerationConfig.from_pretrained(model_id, mirror=\"modelers\")\n",
    "\n",
    "base_model.generation_config.pad_token_id = base_model.generation_config.eos_token_id\n",
    "\n",
    "# LoRA配置\n",
    "config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM, \n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    inference_mode=False,  # 训练模式\n",
    "    r=8,  # Lora 秩\n",
    "    lora_alpha=32,  # Lora alaph，具体作用参见 Lora 原理\n",
    "    lora_dropout=0.1  # Dropout 比例\n",
    ")\n",
    "\n",
    "# 实例化LoRA模型\n",
    "model = get_peft_model(base_model, config)\n",
    "# 获取模型参与训练的参数，发现仅占总参数量的0.5%\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d5b251af-8fee-4fe0-98e0-9d5dfd632e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callback函数，随save_steps定义的步数保存LoRA adapter权重\n",
    "class SavePeftModelCallback(TrainerCallback):\n",
    "    def on_save(\n",
    "        self,\n",
    "        args: TrainingArguments,\n",
    "        state: TrainerState,\n",
    "        control: TrainerControl,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        # LoRA adapter权重保存路径\n",
    "        checkpoint_folder = os.path.join(\n",
    "            args.output_dir, f\"{PREFIX_CHECKPOINT_DIR}-{state.global_step}\"\n",
    "        )\n",
    "\n",
    "        # 保存LoRA adapter权重\n",
    "        peft_model_path = os.path.join(checkpoint_folder, \"adapter_model\")\n",
    "        kwargs[\"model\"].save_pretrained(peft_model_path, safe_serialization=True)\n",
    "\n",
    "        # 移除额外保存的base model的model.safetensors，节约空间\n",
    "        base_model_path = os.path.join(checkpoint_folder, SAFE_WEIGHTS_NAME)\n",
    "        os.remove(base_model_path) if os.path.exists(base_model_path) else None\n",
    "\n",
    "        return control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "38c6f0f7-95db-4352-ba19-33de7276410b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7cdfe4a11044bad911835c3d18c762f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5478, 'learning_rate': 6.666666666666667e-05, 'epoch': 0.33}\n",
      "{'loss': 1.3801, 'learning_rate': 3.3333333333333335e-05, 'epoch': 0.67}\n",
      "{'loss': 1.0026, 'learning_rate': 0.0, 'epoch': 1.0}\n",
      "{'train_runtime': 29.479, 'train_samples_per_second': 0.102, 'train_steps_per_second': 0.102, 'train_loss': 1.3101767698923747, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3, training_loss=1.3101767698923747, metrics={'train_runtime': 29.479, 'train_samples_per_second': 0.102, 'train_steps_per_second': 0.102, 'train_loss': 1.3101767698923747, 'epoch': 1.0})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 训练超参\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"./output/DeepSeek-R1-Distill-Qwen-1.5B\",  # 输出保存路径\n",
    "    per_device_train_batch_size=1,  # batch size\n",
    "    logging_steps=1,  # 每多少步记录一次训练日志\n",
    "    num_train_epochs=1,  # epoch数\n",
    "    save_steps=3,  # 每多少步保存一次权重\n",
    "    learning_rate=1e-4,  # 学习率\n",
    ")\n",
    "\n",
    "# 定义Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=truncated_dataset,\n",
    "    callbacks=[SavePeftModelCallback],\n",
    ")\n",
    "\n",
    "# 启动微调\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
