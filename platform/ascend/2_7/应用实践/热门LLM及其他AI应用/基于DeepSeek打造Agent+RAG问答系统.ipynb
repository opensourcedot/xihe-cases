{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77a5a292",
   "metadata": {},
   "source": [
    "# 📘基于 DeepSeek 打造 Agent+RAG 问答系统\n",
    "感谢 [SeasonMay](https://github.com/opensourcedot/xihe-cases/pull/37) 对本文的贡献。\n",
    "\n",
    "## 背景介绍\n",
    "DeepSeek（杭州深度求索人工智能基础技术研究有限公司）成立于 2023 年 7 月 17 日，是一家专注于大语言模型（LLM）及相关技术研发的创新型科技公司。公司由知名私募机构幻方量化孵化，凭借数据蒸馏技术，提炼出更高质量、更具价值的数据，以提升模型的训练效率和性能。\n",
    "## 技术演进\n",
    "自 2024 年以来，DeepSeek 持续推进大模型的迭代升级，先后发布了以下关键模型：\n",
    "\n",
    "* 2024 年 1 月至 6 月：推出 DeepSeek-LLM、DeepSeek-Coder、DeepSeekMath、DeepSeek-VL、DeepSeek-V2、DeepSeek-Coder-V2；\n",
    "* 2024 年 9 月 5 日：整合 DeepSeek-Coder-V2 与 DeepSeek-V2-Chat，发布 DeepSeek-V2.5；\n",
    "* 2024 年 12 月 13 日：发布多模态模型 DeepSeek-VL2；\n",
    "* 2024 年 12 月 26 日：正式开源旗舰模型 DeepSeek-V3；\n",
    "* 2025 年 1 月 20 日：正式开源 DeepSeek-R1；\n",
    "* 2025 年 5 月 28 日：DeepSeek-R1 进行小版本升级（DeepSeek-R1-0528）整体表现上已接近其他国际顶尖模型，如 o3 与 Gemini-2.5-Pro。\n",
    "\n",
    "这些成果体现了 DeepSeek 在通用语言建模、代码生成、多模态理解等方向的强大研发能力。\n",
    "\n",
    "## 项目介绍\n",
    "本项目基于 DeepSeek 最新的大语言模型，结合**Agent 和 RAG（检索增强生成）** 技术，构建一个智能问答系统，自动解析和回答《MindSpore 设计概览》PDF 文档中的内容。\n",
    "\n",
    "系统具备以下特点：\n",
    "* 📄 支持长文档处理与信息提取\n",
    "* 🔍 使用向量检索提高回答的准确性和相关性\n",
    "* 🧠 通过 Agent 机制自动完成任务拆解与推理\n",
    "* ⚙️ 基于 MindSpore NLP 套件一键部署，轻量实用\n",
    "\n",
    "该系统不仅展示了大语言模型在专业领域文档理解和信息提取方面的能力，同时也为开发者提供了一个实践范例：结合预训练模型与特定领域知识，打造高效的问答解决方案。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f43f46",
   "metadata": {},
   "source": [
    "### ✅ Step 1：安装依赖包\n",
    "本项目主要基于 MindSpore NLP 和 Mindspore 进行开发，构建面向专业文档的问答系统。\n",
    "\n",
    "**MindSpore NLP** 是一个基于 MindSpore 深度学习框架的开源自然语言处理（NLP）库，旨在为各种 NLP 任务提供高效、灵活的解决方案。该库集成了众多常用的 NLP 模型和方法，使研究人员和开发者能够更便捷地构建、训练和部署 NLP 应用，加速技术创新和落地。\n",
    "\n",
    "在开始构建项目之前，请先安装以下依赖项："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b9848f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture captured_output\n",
    "# 实验环境已经预装了mindspore==2.6.0，如需更换mindspore版本，可更改下面 MINDSPORE_VERSION 变量\n",
    "!pip uninstall mindspore -y\n",
    "%env MINDSPORE_VERSION=2.6.0\n",
    "!pip install https://ms-release.obs.cn-north-4.myhuaweicloud.com/${MINDSPORE_VERSION}/MindSpore/unified/aarch64/mindspore-${MINDSPORE_VERSION}-cp39-cp39-linux_aarch64.whl --trusted-host ms-release.obs.cn-north-4.myhuaweicloud.com -i https://pypi.tuna.tsinghua.edu.cn/simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9a60762",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: mindspore\n",
      "Version: 2.6.0\n",
      "Summary: MindSpore is a new open source deep learning training/inference framework that could be used for mobile, edge and cloud scenarios.\n",
      "Home-page: https://www.mindspore.cn\n",
      "Author: The MindSpore Authors\n",
      "Author-email: contact@mindspore.cn\n",
      "License: Apache 2.0\n",
      "Location: /home/mindspore/miniconda/envs/jupyter/lib/python3.9/site-packages\n",
      "Requires: asttokens, astunparse, dill, numpy, packaging, pillow, protobuf, psutil, safetensors, scipy\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "# 查看当前 mindspore 版本\n",
    "!pip show mindspore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac97bb42",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture captured_output\n",
    "!pip install pymupdf sentence-transformers faiss-cpu mindnlp==0.4.1 newspaper3k lxml[html_clean] jieba==0.39"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0bcfd0-a43e-4903-bf2d-d2af74d8e344",
   "metadata": {},
   "source": [
    "### ✅ Step 2：RAG——加载 PDF 文档"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da081b03-b2a3-4976-8db1-5ceace6e05cb",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### RAG介绍\n",
    "大语言模型（LLMs） 在智能交互、语言理解等任务中展现出强大的能力，但在实际业务场景中仍面临以下挑战：\n",
    "\n",
    "1. **知识更新滞后**\n",
    "\n",
    "    大模型的知识来源受限于其训练数据，当前主流大模型（如 ChatGPT、文心一言、通义千问等）主要基于网络公开数据进行训练，导致难以获取实时、私有或离线数据，意味着它们往往难以获取实时、非公开或离线数据中的知识，从而限制了模型在某些专业领域的应用。\n",
    "\n",
    "2. **幻觉问题（Hallucination）**\n",
    "\n",
    "    由于 LLM 本质上基于概率建模，输出实质上是一系列数值运算的结果。在某些情况下，这可能导致模型在不擅长的场景或缺乏相关知识时产生误导性的回答。这种幻觉问题的识别需要使用者具备相应领域的知识，从而限制了使用的效果。\n",
    "\n",
    "3. **数据安全与隐私**\n",
    "\n",
    "    数据安全性也是现代社会关注的焦点，企业或个人通常不愿上传私有数据到第三方平台，担心数据泄露风险。这也进一步限制了大模型在企业级业务中的落地能力。\n",
    "\n",
    "为了解决上述问题，检索增强生成（Retrieval Augmented Generation ，RAG）技术应运而生。它将信息检索与自然语言生成相结合，让大模型不仅依赖其“记忆”，还能借助外部知识库来回答问题。\n",
    "\n",
    "RAG 系统核心结构可概括为两部分：\n",
    "> 向量检索（Retrieval） + 大模型生成（Generation）\n",
    "\n",
    "在向量检索部分，系统使用向量数据库对文档进行编码与检索；而在生成部分，借助大模型对补充背景信息的用户输入进行语义理解与答案生成。\n",
    "\n",
    "![图片1](https://matuimg.com/i/2025/03/31/10ovsfn.png)\n",
    "\n",
    "**RAG 工作流程如下**：\n",
    "1. 检索阶段\n",
    "\n",
    "    用户提出问题后，系统首先从向量数据库中检索出与查询高度相关的文档片段。\n",
    "\n",
    "2. 生成阶段\n",
    "\n",
    "    检索结果与用户原始输入被拼接后作为上下文输入大模型，生成包含真实信息的高质量回答。\n",
    "\n",
    "这一机制可显著降低幻觉风险，提升回答的准确性和可控性。\n",
    "\n",
    "在本案例中，我们基于《MindSpore 设计概览》PDF 文档构建知识库，通过 RAG 技术实现对该文档内容的智能问答。\n",
    "\n",
    "你可以使用以下代码下载并加载该 PDF 文档以构建本地语义索引，用于后续的向量检索和问答生成任务："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ab5f0c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/white_paper/MindSpore_white_paperV1.1.pdf (1.7 MB)\n",
      "\n",
      "file_sizes: 100%|███████████████████████████| 1.83M/1.83M [00:00<00:00, 210MB/s]\n",
      "Successfully downloaded file to MindSpore_Design_Overview.pdf\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'MindSpore_Design_Overview.pdf'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from download import download\n",
    "\n",
    "url = \"https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/white_paper/MindSpore_white_paperV1.1.pdf\"\n",
    "\n",
    "# 下载 《MindSpore 设计概览》 PDF 文档\n",
    "download(url, \"MindSpore_Design_Overview.pdf\", replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42560d9d-823c-4032-83f0-e2668b119b10",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "pdf_path = 'MindSpore_Design_Overview.pdf'\n",
    "\n",
    "doc = fitz.open(pdf_path)\n",
    "pdf_text = [page.get_text() for page in doc]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef95089",
   "metadata": {},
   "source": [
    "### ✅ Step 3：清洗 PDF 文本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d93daeb5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 定义文本清洗函数：将文本按空格切分并重新连接，去除多余空白字符\n",
    "def clean_text(text):\n",
    "    return ' '.join(text.split())\n",
    "\n",
    "# 构建一个 DataFrame，用于存储每页 PDF 的页码与对应的清洗后文本\n",
    "pdf_df = pd.DataFrame({\n",
    "    'page': list(range(1, len(pdf_text)+1)),\n",
    "    'text': [clean_text(t) for t in pdf_text]\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5bbead1",
   "metadata": {},
   "source": [
    "### ✅ Step 4：切分为语义块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d3151230",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chunks = []\n",
    "# 遍历每一页的文本内容\n",
    "for _, row in pdf_df.iterrows():\n",
    "    text = row['text']\n",
    "    # 按固定长度（300 个字符）对文本进行滑动窗口式切分\n",
    "    for i in range(0, len(text), 300):\n",
    "        chunk = text[i:i+300]\n",
    "        chunks.append(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9dd2164",
   "metadata": {},
   "source": [
    "### ✅ Step 5：向量化并构建 FAISS 检索器"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0d1b3d-0017-47c6-8517-bceab194698b",
   "metadata": {},
   "source": [
    "在实际应用中，文档往往包含成千上万个字符，超出大语言模型的上下文窗口限制，直接输入整篇内容可能导致关键信息被截断、理解不完整，影响问答准确率。\n",
    "\n",
    "为解决这一问题，常用策略是将文档按语义或固定长度进行切分，生成多个小片段（chunk）。此外，可设置段间字符重叠，增强上下文连贯性，提升模型对语义单元的感知能力，从而提高信息提取的准确性和鲁棒性。\n",
    "\n",
    "#### 文本向量化\n",
    "切分后的文本片段将通过嵌入模型转化为稠密向量（embedding），使其具备可计算的语义表示。这些向量可参与后续的相似度检索。常见向量化模型包括：\n",
    "* Openai的ChatGPT-Embedding\n",
    "* 百度的ERNIE-Embedding V1\n",
    "* 北京智源研究院推出的 BGE 模型\n",
    "* 本项目采用的轻量级模型 paraphrase-multilingual-MiniLM-L12-v2\n",
    "\n",
    "#### 向量索引构建原理\n",
    "为了避免在检索阶段对所有向量进行暴力全量比对，系统需对文本向量构建高效的索引结构，支持快速查找相似文本片段。\n",
    "\n",
    "向量索引的核心技术为近似最近邻搜索（ANN），常见构建方式包括：\n",
    "1. 空间划分类\n",
    "\n",
    "    将向量空间分成多个区域，仅在最相关区域内进行比对，加快搜索速度：\n",
    "    * IVF（倒排文件索引）\n",
    "    * HNSW（分层近邻图）\n",
    "2. 距离度量类\n",
    "\n",
    "    通过向量间距离度量来评估相似度，常用指标包括：\n",
    "    * L2（欧氏距离）\n",
    "    * Inner Product（内积）\n",
    "    * Cosine Similarity（余弦相似度）：衡量语义一致性\n",
    "\n",
    "#### 向量数据库选择\n",
    "在实际部署中，我们通常将文本向量存入向量数据库中，构建索引以支持高速检索，常用的向量数据库包括：\n",
    "* FAISS：轻量、适用于本地部署的小型向量库\n",
    "* ChromaDB：支持嵌入文档管理的轻量数据库\n",
    "* Elasticsearch：传统搜索引擎，支持向量检索插件\n",
    "* Milvus：面向大规模向量数据的分布式向量数据库\n",
    "\n",
    "在选择适合的数据库时，应全面考虑业务的具体需求、硬件配置以及性能要求等诸多因素，以确保选出最适合的数据库方案。本项目采用较通用常见的**FAISS**进行向量入库与索引构建，以满足本地化部署和快速原型开发的需求。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "33ebd8ce-b2de-43b9-b74f-ed740934d800",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Git LFS initialized.\n",
      "正克隆到 'paraphrase-multilingual-MiniLM-L12-v2'...\n",
      "remote: Enumerating objects: 25, done.\u001B[K\n",
      "remote: Counting objects: 100% (25/25), done.\u001B[K\n",
      "remote: Compressing objects: 100% (22/22), done.\u001B[K\n",
      "remote: Total 25 (delta 1), reused 0 (delta 0), pack-reused 0\u001B[K\n",
      "接收对象中: 100% (25/25), 6.97 MiB | 10.93 MiB/s, 完成.\n",
      "处理 delta 中: 100% (1/1), 完成.\n",
      "过滤内容: 100% (3/3), 902.81 MiB | 14.38 MiB/s, 完成.\n"
     ]
    }
   ],
   "source": [
    "### 建议下载到本地部署，可以从魔塔或者昇思大模型平台下载\n",
    "!git lfs install\n",
    "!git clone https://www.modelscope.cn/Ceceliachenen/paraphrase-multilingual-MiniLM-L12-v2.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a266ef1-442e-4610-b040-7ec618ae17d3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mindspore/miniconda/envs/jupyter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "# 加载 SentenceTransformer 嵌入模型\n",
    "embed_model = SentenceTransformer('./paraphrase-multilingual-MiniLM-L12-v2')\n",
    "# 将每个文本片段（chunk）编码为稠密向量（embedding）\n",
    "chunk_embeddings = embed_model.encode(chunks)\n",
    "\n",
    "# 构建一个基于 L2 距离的向量索引器\n",
    "index = faiss.IndexFlatL2(chunk_embeddings.shape[1])\n",
    "# 将所有文本片段的向量添加到索引中\n",
    "index.add(np.array(chunk_embeddings))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf67a895",
   "metadata": {},
   "source": [
    "### ✅ Step 6：定义检索接口"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f34b5844",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def query_pdf(query, top_k=3):\n",
    "    # 将用户输入的问题（query）编码为向量\n",
    "    q_vec = embed_model.encode([query])\n",
    "    \n",
    "    # 在向量索引中查找与查询向量最相近的 top_k 个文档片段\n",
    "    D, I = index.search(np.array(q_vec), top_k)    # D：距离，I：索引列表\n",
    "    \n",
    "    # 根据检索结果的索引，从原始 chunks 中提取对应的文本片段\n",
    "    return [chunks[i] for i in I[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d6f05c",
   "metadata": {},
   "source": [
    "### ✅ Step 7：加载 Deepseek 模型\n",
    "本项目采用了 DeepSeek-R1-Distill-Qwen-7B 模型 —— 一种经过蒸馏压缩的小规模大语言模型，具备较高的推理效率和良好的语言理解能力，适用于资源有限场景下的智能问答任务。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bb635439",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache /tmp/jieba.cache\n",
      "Loading model cost 1.048 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "641758c056a842c582811765f437ab73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/3.00k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "591bf025dd0e4483b043b7e38d0654dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/6.71M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b47cb66ae57b4bd6b67881d6603a1cae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/680 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e5ab7a93bff4e6e9e22cee7ac50fbc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/27.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cff2ccb590c84fd59e50e866f096660f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a2ad03d23ca48bfa06f5d8400588687",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/8.02G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a125047321024c028a3186e956616425",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/6.17G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Qwen2ForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`.`PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MS_ALLOC_CONF]Runtime config:  enable_vmm:True  vmm_align_size:2MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `eager`; unexpected results may be encountered.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48dd3992320a44d2a52665e8cb7c036e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d21e074b40e425db95e06f751da70ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/181 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import mindspore as ms\n",
    "from mindnlp.transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# 加载模型\n",
    "model_id = \"MindSpore-Lab/DeepSeek-R1-Distill-Qwen-7B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, mirror='modelers', ms_dtype=ms.float16)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, mirror='modelers', ms_dtype=ms.float16)\n",
    "# 设置为推理模式\n",
    "model.set_train(False)\n",
    "\n",
    "def qwen_generate(prompt):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    inputs = tokenizer([text], return_tensors=\"ms\")\n",
    "    # 调用模型的 generate 方法，生成新的 tokens\n",
    "    outputs = model.generate(**inputs, max_new_tokens=256)\n",
    "    # 去除原始输入部分，只保留模型生成的内容\n",
    "    output = [o[len(i):] for i, o in zip(inputs.input_ids, outputs)]\n",
    "    # 解码为可读文本\n",
    "    return tokenizer.batch_decode(output, skip_special_tokens=True)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18834ed3",
   "metadata": {},
   "source": [
    "### ✅ Step 8：集成工具：时间、翻译、网页摘要\n",
    "\n",
    "在人类文明的发展历程中，工具的发明与使用始终代表着智慧的跃升。工具不仅扩展了人类的能力，也极大地提升了我们解决复杂问题的效率与生产力。随着任务复杂度不断提升，我们更依赖于工具的辅助，以腾出时间与资源，专注于更具价值的目标。\n",
    "\n",
    "同样地，大语言模型（LLMs）虽然具备丰富的通识知识与优秀的语言理解与生成能力，但在真实场景中仍面临一些局限，例如：\n",
    "* 预训练数据具有滞后性，无法感知最新动态信息；\n",
    "* 私域数据受限于安全与隐私要求，无法直接访问；\n",
    "* 存在幻觉（hallucination），生成内容缺乏依据或可信度。\n",
    "\n",
    "即便引入了 RAG（检索增强生成）机制，在一些场景中依然难以满足复杂任务的实时性和准确性需求。因此，正如人类依赖工具拓展自身能力，大模型也需要具备**调用外部工具**的能力，进一步增强其实用性与交互性。引入外部工具调用机制，可以带来以下几方面的增强：\n",
    "* 接入动态数据源（如当前时间、网页内容），提升输出的实时性与上下文关联性；\n",
    "* 打破单一文本生成范式，实现跨模态、多任务集成（如结构化摘要、翻译、表格处理等）；\n",
    "* 增强模型交互性与任务执行力，从单一问答助手进化为可执行 agent。\n",
    "\n",
    "通过工具的集成，大模型不再只是一个被动响应的**语言系统**，而真正成为一个具备主动任务能力的**智能体（Agent）**。\n",
    "\n",
    "为了展示 LLM 与工具协作的基本能力，本项目集成了以下几类通用工具，供智能体按需调用：\n",
    "\n",
    "* **时间查询工具**：用于提供当前系统时间，解决时效性问题\n",
    "* **网页摘要工具**：抓取指定网页内容并生成摘要\n",
    "* **中英互译工具**：用于对话或内容的实时翻译"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2c72e88c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from newspaper import Article\n",
    "from datetime import datetime, timezone, timedelta\n",
    "\n",
    "\n",
    "def get_time_info(query):\n",
    "    # 设置为 UTC+8 时区的时间\n",
    "    tz_utc_8 = timezone(timedelta(hours=8))\n",
    "    now = datetime.now(tz_utc_8)  # 获取带时区信息的当前时间\n",
    "\n",
    "    if \"几点\" in query:\n",
    "        return f\"现在是 {now.strftime('%Y-%m-%d %H:%M:%S')}\"\n",
    "    elif \"星期\" in query:\n",
    "        weekdays = [\"一\", \"二\", \"三\", \"四\", \"五\", \"六\", \"日\"]\n",
    "        weekday_cn = weekdays[now.isoweekday() - 1]  # isoweekday(): 1~7（周一到周日）\n",
    "        return f\"今天是星期{weekday_cn}，{now.strftime('%Y-%m-%d')}\"\n",
    "    elif \"几号\" in query or \"日期\" in query:\n",
    "        return f\"今天是 {now.strftime('%Y-%m-%d')}\"\n",
    "    \n",
    "    # 默认提示信息\n",
    "    return \"我可以告诉你时间哦~\"\n",
    "\n",
    "\n",
    "def translate(query):\n",
    "    if \"翻译\" in query:\n",
    "        # 确定翻译方向（中译英/英译中）\n",
    "        target = \"英文\" if \"英文\" in query else \"中文\"\n",
    "        # 清理查询中的翻译指令关键词\n",
    "        content = query.replace(\"翻译\", \"\").replace(\"成英文\", \"\").replace(\"成中文\", \"\")\n",
    "        # 调用模型进行翻译\n",
    "        return qwen_generate(f\"请将以下内容翻译成{target}：{content}\")\n",
    "    return \"翻译请求格式不清晰。\"\n",
    "\n",
    "\n",
    "def summarize_web(url):\n",
    "    try:\n",
    "        # 创建文章对象并设置语言为中文\n",
    "        article = Article(url, language='zh')\n",
    "        # 下载并解析网页内容\n",
    "        article.download()\n",
    "        article.parse()\n",
    "        # 调用Qwen模型进行摘要生成\n",
    "        return qwen_generate(\"请总结以下网页内容：\" + article.text[:2000])\n",
    "    except Exception as e:\n",
    "        return f\"❌ 网页摘要失败：{e}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b87730f",
   "metadata": {},
   "source": [
    "### ✅ Step 9：Agent 路由控制器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0af5bd7d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PDFAgent:\n",
    "    def answer(self, query):\n",
    "        \"\"\"\n",
    "        根据用户输入内容智能分发处理任务\n",
    "        处理逻辑分支:\n",
    "        1. 数学计算 → 调用Qwen模型求解\n",
    "        2. 时间查询 → 调用时间信息模块\n",
    "        3. 翻译请求 → 调用翻译模块\n",
    "        4. 网页链接 → 生成网页摘要\n",
    "        5. 其他问题 → PDF文档检索+Qwen生成答案\n",
    "        \"\"\"\n",
    "        if any(k in query for k in [\"计算\", \"+\", \"-\", \"*\", \"/\"]):\n",
    "            return qwen_generate(f\"请计算：{query}\")\n",
    "        elif any(k in query for k in [\"几点\", \"星期\", \"几号\"]):\n",
    "            return get_time_info(query)\n",
    "        elif \"翻译\" in query:\n",
    "            return translate(query)\n",
    "        elif query.startswith(\"http\"):\n",
    "            return summarize_web(query)\n",
    "        else:\n",
    "            # 1. 从PDF库检索相关内容\n",
    "            retrieved = \"\\n\".join(query_pdf(query))\n",
    "            # 2. 结合上下文生成回答\n",
    "            return qwen_generate(f\"以下是相关资料：{retrieved}\\n请回答：{query}\")\n",
    "\n",
    "agent = PDFAgent()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ecde6c",
   "metadata": {},
   "source": [
    "### ✅ Step 10：测试示例"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284f4be5-f56d-4515-b1b9-0755717484e6",
   "metadata": {},
   "source": [
    "#### RAG知识库背景下的问答示例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a27df70f-2286-4d14-b11c-c934fb65fbe6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "嗯，用户问的是MindSpore是什么。根据提供的资料，MindSpore是一个深度学习的计算框架，目标是易开发、高效执行和全场景覆盖。它使用了源码转换的自动微分机制，这个机制能处理复杂的控制流，把函数转换成中间表达，然后构建计算图，再应用软硬件优化提升性能。另外，它支持静态编译优化，性能不错。还提到了几个主要组件：MindExpression、MindCompiler、MindData、MindRE和MindArmour。技术贡献包括高效执行、端到端训练、模型自定义和性能优化。\n",
      "\n",
      "我需要把这些信息整理成一个清晰、简洁的回答，确保涵盖所有关键点，同时语言通顺易懂。可能用户是想了解MindSpore的基本概念和主要功能，所以我会简要介绍其核心机制、目标以及组件，最后提到它的技术贡献，这样用户能全面了解MindSpore是什么。\n",
      "</think>\n",
      "\n",
      "MindSpore 是一种全场景覆盖的深度学习计算框架，旨在实现易开发、高效执行和全场景覆盖三大目标。它采用基于源码转换（Source Code Transformation，SCT）的自动微分（Automatic Differentiation，AD）机制，支持复杂的\n"
     ]
    }
   ],
   "source": [
    "print(agent.answer(\"什么是MindSpore？\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323965e3-3c12-4676-9d88-3a8630b1df10",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 时间工具调用的示例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "47a689d6-dd81-498a-80dd-5ce0dc431184",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "现在是 2025-07-01 11:17:20\n"
     ]
    }
   ],
   "source": [
    "print(agent.answer(\"现在几点？\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "99b54226-931c-4b0b-966b-6183aeaca24c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "今天是星期二，2025-07-01\n"
     ]
    }
   ],
   "source": [
    "print(agent.answer(\"今天是星期几？\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c6700c-a76b-4d2c-bf54-5efd37dbc5bd",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 中英互译工具调用的示例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b8dea65e-322b-46b9-ab43-c5eb850754c5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "好，我现在需要把用户提供的中文内容翻译成英文。用户提供的内容是：“你好，我是AI助手”。首先，我要理解这句话的意思。这句话通常用于自我介绍，可能是在社交媒体或者聊天软件中使用。\n",
      "\n",
      "接下来，我要确定翻译的准确性和自然流畅性。中文里的“你好”可以翻译成“Hello”，这是最常用的问候语。然后，“我是AI助手”这部分，直接翻译的话就是“I am an AI Assistant”。“AI”指的是“Artificial Intelligence”，所以直接写成缩写“AI”就可以了。\n",
      "\n",
      "组合起来就是“Hello, I am an AI Assistant.” 这样不仅准确，而且符合英语的表达习惯。再检查一下语法和用词是否正确，确保没有错误。\n",
      "\n",
      "另外，考虑到用户可能是想用这句话来介绍自己，所以保持语气友好和专业是关键。因此，翻译后的句子应该简洁明了，同时体现出AI助手的专业性。\n",
      "\n",
      "总结一下，翻译过程就是理解内容，选择合适的英文词汇，确保语法正确，并且符合目标语言的表达习惯。最终的翻译结果应该是“Hello, I am an AI Assistant.”\n",
      "</think>\n",
      "\n",
      "Hello, I am an AI Assistant.\n"
     ]
    }
   ],
   "source": [
    "print(agent.answer(\"翻译成英文：你好，我是AI助手\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
