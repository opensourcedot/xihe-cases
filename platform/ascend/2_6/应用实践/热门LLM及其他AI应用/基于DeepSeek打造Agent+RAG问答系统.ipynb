{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77a5a292",
   "metadata": {},
   "source": [
    "# 📘基于 DeepSeek 打造 Agent+RAG 问答系统\n",
    "感谢 [SeasonMay](https://github.com/opensourcedot/xihe-cases/pull/37) 对本文的贡献。\n",
    "\n",
    "## 背景介绍\n",
    "DeepSeek（杭州深度求索人工智能基础技术研究有限公司）成立于 2023 年 7 月 17 日，是一家专注于大语言模型（LLM）及相关技术研发的创新型科技公司。公司由知名私募机构幻方量化孵化，凭借数据蒸馏技术，提炼出更高质量、更具价值的数据，以提升模型的训练效率和性能。\n",
    "## 技术演进\n",
    "自 2024 年以来，DeepSeek 持续推进大模型的迭代升级，先后发布了以下关键模型：\n",
    "\n",
    "* 2024 年 1 月至 6 月：推出 DeepSeek-LLM、DeepSeek-Coder、DeepSeekMath、DeepSeek-VL、DeepSeek-V2、DeepSeek-Coder-V2；\n",
    "* 2024 年 9 月 5 日：整合 DeepSeek-Coder-V2 与 DeepSeek-V2-Chat，发布 DeepSeek-V2.5；\n",
    "* 2024 年 12 月 13 日：发布多模态模型 DeepSeek-VL2；\n",
    "* 2024 年 12 月 26 日：正式开源旗舰模型 DeepSeek-V3。\n",
    "\n",
    "这些成果体现了 DeepSeek 在通用语言建模、代码生成、多模态理解等方向的强大研发能力。\n",
    "\n",
    "## 项目介绍\n",
    "本项目基于 DeepSeek 先进的大语言模型，构建一个 **Agent + RAG（Retrieval-Augmented Generation）问答系统**，可智能解析并回答来自《MindSpore 设计概览》PDF 文档中的内容。\n",
    "\n",
    "该系统不仅展示了大语言模型在专业领域文档理解和信息提取方面的能力，同时也为开发者提供了一个实践范例，说明如何结合预训练模型与特定领域知识，打造高效的问答解决方案。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f43f46",
   "metadata": {},
   "source": [
    "### ✅ Step 1：安装依赖包\n",
    "本项目主要基于 MindSpore NLP 和 Mindspore 进行开发，构建面向专业文档的问答系统。\n",
    "\n",
    "**MindSpore NLP** 是一个基于 MindSpore 深度学习框架的开源自然语言处理（NLP）库，旨在为各种 NLP 任务提供高效、灵活的解决方案。该库集成了众多常用的 NLP 模型和方法，使研究人员和开发者能够更便捷地构建、训练和部署 NLP 应用，加速技术创新和落地。\n",
    "\n",
    "在开始构建项目之前，请先安装以下依赖项："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b9848f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture captured_output\n",
    "# 实验环境已经预装了mindspore==2.6.0，如需更换mindspore版本，可更改下面 MINDSPORE_VERSION 变量\n",
    "!pip uninstall mindspore -y\n",
    "%env MINDSPORE_VERSION=2.6.0\n",
    "!pip install https://ms-release.obs.cn-north-4.myhuaweicloud.com/${MINDSPORE_VERSION}/MindSpore/unified/aarch64/mindspore-${MINDSPORE_VERSION}-cp39-cp39-linux_aarch64.whl --trusted-host ms-release.obs.cn-north-4.myhuaweicloud.com -i https://pypi.tuna.tsinghua.edu.cn/simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9a60762",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: mindspore\n",
      "Version: 2.6.0\n",
      "Summary: MindSpore is a new open source deep learning training/inference framework that could be used for mobile, edge and cloud scenarios.\n",
      "Home-page: https://www.mindspore.cn\n",
      "Author: The MindSpore Authors\n",
      "Author-email: contact@mindspore.cn\n",
      "License: Apache 2.0\n",
      "Location: /home/mindspore/miniconda/envs/jupyter/lib/python3.9/site-packages\n",
      "Requires: asttokens, astunparse, dill, numpy, packaging, pillow, protobuf, psutil, safetensors, scipy\n",
      "Required-by: mindnlp\n"
     ]
    }
   ],
   "source": [
    "# 查看当前 mindspore 版本\n",
    "!pip show mindspore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac97bb42",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture captured_output\n",
    "!pip install pymupdf sentence-transformers faiss-cpu mindnlp==0.4.1 newspaper3k lxml[html_clean] jieba==0.39"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0bcfd0-a43e-4903-bf2d-d2af74d8e344",
   "metadata": {},
   "source": [
    "### ✅ Step 2：RAG——加载 PDF 文档"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da081b03-b2a3-4976-8db1-5ceace6e05cb",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### RAG介绍\n",
    "大语言模型（LLMs） 在智能交互、语言理解等任务中展现出强大的能力，但在实际业务场景中仍面临以下挑战：\n",
    "\n",
    "1. **知识更新滞后**\n",
    "\n",
    "    大模型的知识来源受限于其训练数据，当前主流大模型（如 ChatGPT、文心一言、通义千问等）主要基于网络公开数据进行训练，导致难以获取实时、私有或离线数据，意味着它们往往难以获取实时、非公开或离线数据中的知识，从而限制了模型在某些专业领域的应用。\n",
    "\n",
    "2. **幻觉问题（Hallucination）**\n",
    "\n",
    "    由于 LLM 本质上基于概率建模，输出实质上是一系列数值运算的结果。在某些情况下，这可能导致模型在不擅长的场景或缺乏相关知识时产生误导性的回答。这种幻觉问题的识别需要使用者具备相应领域的知识，从而限制了使用的效果。\n",
    "\n",
    "3. **数据安全与隐私**\n",
    "\n",
    "    数据安全性也是现代社会关注的焦点，企业或个人通常不愿上传私有数据到第三方平台，担心数据泄露风险。这也进一步限制了大模型在企业级业务中的落地能力。\n",
    "\n",
    "为了解决上述问题，检索增强生成（Retrieval Augmented Generation ，RAG）技术应运而生。它将信息检索与自然语言生成相结合，让大模型不仅依赖其“记忆”，还能借助外部知识库来回答问题。\n",
    "\n",
    "RAG 系统核心结构可概括为两部分：\n",
    "> 向量检索（Retrieval） + 大模型生成（Generation）\n",
    "\n",
    "在向量检索部分，系统使用向量数据库对文档进行编码与检索；而在生成部分，借助大模型对补充背景信息的用户输入进行语义理解与答案生成。\n",
    "![图片1](https://matuimg.com/i/2025/03/31/10ovsfn.png)\n",
    "\n",
    "**RAG 工作流程如下**：\n",
    "1. 检索阶段\n",
    "\n",
    "    用户提出问题后，系统首先从向量数据库中检索出与查询高度相关的文档片段。\n",
    "\n",
    "2. 生成阶段\n",
    "\n",
    "    检索结果与用户原始输入被拼接后作为上下文输入大模型，生成包含真实信息的高质量回答。\n",
    "\n",
    "这一机制可显著降低幻觉风险，提升回答的准确性和可控性。\n",
    "\n",
    "在本案例中，我们基于《MindSpore 设计概览》PDF 文档构建知识库，通过 RAG 技术实现对该文档内容的智能问答。\n",
    "\n",
    "你可以使用以下代码下载并加载该 PDF 文档以构建本地语义索引，用于后续的向量检索和问答生成任务："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ab5f0c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/white_paper/MindSpore_white_paperV1.1.pdf (1.7 MB)\n",
      "\n",
      "file_sizes: 100%|███████████████████████████| 1.83M/1.83M [00:00<00:00, 192MB/s]\n",
      "Successfully downloaded file to MindSpore_Design_Overview.pdf\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'MindSpore_Design_Overview.pdf'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from download import download\n",
    "\n",
    "url = \"https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/white_paper/MindSpore_white_paperV1.1.pdf\"\n",
    "\n",
    "# 下载 《MindSpore 设计概览》 PDF 文档\n",
    "download(url, \"MindSpore_Design_Overview.pdf\", replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42560d9d-823c-4032-83f0-e2668b119b10",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "pdf_path = 'MindSpore_Design_Overview.pdf'\n",
    "\n",
    "doc = fitz.open(pdf_path)\n",
    "pdf_text = [page.get_text() for page in doc]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef95089",
   "metadata": {},
   "source": "### ✅ Step 3：清洗 PDF 文本"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d93daeb5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 定义文本清洗函数：将文本按空格切分并重新连接，去除多余空白字符\n",
    "def clean_text(text):\n",
    "    return ' '.join(text.split())\n",
    "\n",
    "# 构建一个 DataFrame，用于存储每页 PDF 的页码与对应的清洗后文本\n",
    "pdf_df = pd.DataFrame({\n",
    "    'page': list(range(1, len(pdf_text)+1)),\n",
    "    'text': [clean_text(t) for t in pdf_text]\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5bbead1",
   "metadata": {},
   "source": "### ✅ Step 4：切分为语义块"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d3151230",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chunks = []\n",
    "# 遍历每一页的文本内容\n",
    "for _, row in pdf_df.iterrows():\n",
    "    text = row['text']\n",
    "    # 按固定长度（300 个字符）对文本进行滑动窗口式切分\n",
    "    for i in range(0, len(text), 300):\n",
    "        chunk = text[i:i+300]\n",
    "        chunks.append(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9dd2164",
   "metadata": {},
   "source": "### ✅ Step 5：向量化并构建 FAISS 检索器"
  },
  {
   "cell_type": "markdown",
   "id": "ee0d1b3d-0017-47c6-8517-bceab194698b",
   "metadata": {},
   "source": [
    "在实际应用中，文档往往包含成千上万的字符，对于大语言模型而言，直接处理整篇长文本可能面临上下文长度限制，进而影响理解效果，尤其在提取关键信息时可能出现遗漏或误判。为应对这一挑战，常用的做法是采用**文本切分策略**，将大型文档拆分为多个较小的片段（chunk）。\n",
    "\n",
    "本项目中，我们也采用了这一方法：将原始文档划分为多个小段，每段包含固定数量的字符，并在段与段之间引入部分字符的重叠。这样可以在避免上下文过长的同时，确保一个完整语义单元的上下文不会因切分而被截断，从而提升后续问答系统的信息提取准确率和鲁棒性。\n",
    "\n",
    "完成文本切分后，我们对每个文本块进行向量化处理，即将自然语言内容转换为稠密向量（embedding），从而使其可以参与后续的相似度检索。向量化模型众多，包括：\n",
    "* Openai的ChatGPT-Embedding\n",
    "* 百度的ERNIE-Embedding V1\n",
    "* 北京智源研究院推出的 BGE 模型\n",
    "* 以及本项目采用的轻量级模型 paraphrase-multilingual-MiniLM-L12-v2\n",
    "\n",
    "文本向量生成后，我们会将所有向量存入一个**向量数据库**中，并构建索引结构，以便在系统运行时根据用户问题快速检索出最相关的文本片段，实现“以文搜文”的功能。这一过程，也被称作**数据入库**。在 RAG（Retrieval-Augmented Generation，检索增强生成）系统中，常用的向量数据库包括：\n",
    "* FAISS：轻量、适用于本地部署的小型向量库\n",
    "* ChromaDB：支持嵌入文档管理的轻量数据库\n",
    "* Elasticsearch：传统搜索引擎，支持向量检索插件\n",
    "* Milvus：面向大规模向量数据的分布式向量数据库\n",
    "\n",
    "在选择适合的数据库时，应全面考虑业务的具体需求、硬件配置以及性能要求等诸多因素，以确保选出最适合的数据库方案。本项目采用较通用常见的**FAISS**进行向量入库与索引构建，以满足本地化部署和快速原型开发的需求。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "33ebd8ce-b2de-43b9-b74f-ed740934d800",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Git LFS initialized.\n",
      "fatal: 目标路径 'paraphrase-multilingual-MiniLM-L12-v2' 已经存在，并且不是一个空目录。\n"
     ]
    }
   ],
   "source": [
    "### 建议下载到本地部署，可以从魔塔或者昇思大模型平台下载\n",
    "!git lfs install\n",
    "!git clone https://www.modelscope.cn/Ceceliachenen/paraphrase-multilingual-MiniLM-L12-v2.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a266ef1-442e-4610-b040-7ec618ae17d3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mindspore/miniconda/envs/jupyter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "# 加载 SentenceTransformer 嵌入模型\n",
    "embed_model = SentenceTransformer('./paraphrase-multilingual-MiniLM-L12-v2')\n",
    "# 将每个文本片段（chunk）编码为稠密向量（embedding）\n",
    "chunk_embeddings = embed_model.encode(chunks)\n",
    "\n",
    "# 构建一个基于 L2 距离的向量索引器\n",
    "index = faiss.IndexFlatL2(chunk_embeddings.shape[1])\n",
    "# 将所有文本片段的向量添加到索引中\n",
    "index.add(np.array(chunk_embeddings))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf67a895",
   "metadata": {},
   "source": "### ✅ Step 6：定义检索接口"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f34b5844",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def query_pdf(query, top_k=3):\n",
    "    # 将用户输入的问题（query）编码为向量\n",
    "    q_vec = embed_model.encode([query])\n",
    "    \n",
    "    # 在向量索引中查找与查询向量最相近的 top_k 个文档片段\n",
    "    D, I = index.search(np.array(q_vec), top_k)    # D：距离，I：索引列表\n",
    "    \n",
    "    # 根据检索结果的索引，从原始 chunks 中提取对应的文本片段\n",
    "    return [chunks[i] for i in I[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d6f05c",
   "metadata": {},
   "source": [
    "### ✅ Step 7：加载 Deepseek 模型\n",
    "本项目采用了 DeepSeek-R1-Distill-Qwen-7B 模型 —— 一种经过蒸馏压缩的小规模大语言模型，具备较高的推理效率和良好的语言理解能力，适用于资源有限场景下的智能问答任务。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bb635439",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 1.076 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "Qwen2ForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`.`PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MS_ALLOC_CONF]Runtime config:  enable_vmm:True  vmm_align_size:2MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `eager`; unexpected results may be encountered.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ad48cd0426e46f383f2d71ba521cbf3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import mindspore as ms\n",
    "from mindnlp.transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# 加载模型\n",
    "model_id = \"MindSpore-Lab/DeepSeek-R1-Distill-Qwen-7B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, mirror='modelers', ms_dtype=ms.float16)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, mirror='modelers', ms_dtype=ms.float16)\n",
    "# 设置为推理模式\n",
    "model.set_train(False)\n",
    "\n",
    "def qwen_generate(prompt):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    inputs = tokenizer([text], return_tensors=\"ms\")\n",
    "    # 调用模型的 generate 方法，生成新的 tokens\n",
    "    outputs = model.generate(**inputs, max_new_tokens=256)\n",
    "    # 去除原始输入部分，只保留模型生成的内容\n",
    "    output = [o[len(i):] for i, o in zip(inputs.input_ids, outputs)]\n",
    "    # 解码为可读文本\n",
    "    return tokenizer.batch_decode(output, skip_special_tokens=True)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18834ed3",
   "metadata": {},
   "source": [
    "### ✅ Step 8：集成工具：时间、翻译、网页摘要\n",
    "\n",
    "在人类文明的发展历程中，工具的发明与使用始终代表着智慧的跃升。工具不仅扩展了人类的能力，也极大地提升了我们解决复杂问题的效率与生产力。随着任务复杂度不断提升，我们更依赖于工具的辅助，以腾出时间与资源，专注于更具价值的目标。\n",
    "\n",
    "同样地，大语言模型（LLMs）虽然具备丰富的通识知识与优秀的语言理解与生成能力，但在真实场景中仍面临一些局限，例如：\n",
    "* 预训练数据具有滞后性，无法感知最新动态信息；\n",
    "* 私域数据受限于安全与隐私要求，无法直接访问；\n",
    "* 存在幻觉（hallucination），生成内容缺乏依据或可信度。\n",
    "\n",
    "即便引入了 RAG（检索增强生成）机制，在一些场景中依然难以满足复杂任务的实时性和准确性需求。因此，正如人类依赖工具拓展自身能力，大模型也需要具备**调用外部工具**的能力，进一步增强其实用性与交互性。引入外部工具调用机制，可以带来以下几方面的增强：\n",
    "* 接入动态数据源（如当前时间、网页内容），提升输出的实时性与上下文关联性；\n",
    "* 打破单一文本生成范式，实现跨模态、多任务集成（如结构化摘要、翻译、表格处理等）；\n",
    "* 增强模型交互性与任务执行力，从单一问答助手进化为可执行 agent。\n",
    "\n",
    "通过工具的集成，大模型不再只是一个被动响应的**语言系统**，而真正成为一个具备主动任务能力的**智能体（Agent）**。\n",
    "\n",
    "为了展示 LLM 与工具协作的基本能力，本项目集成了以下几类通用工具，供智能体按需调用：\n",
    "\n",
    "* **时间查询工具**：用于提供当前系统时间，解决时效性问题\n",
    "* **网页摘要工具**：抓取指定网页内容并生成摘要\n",
    "* **中英互译工具**：用于对话或内容的实时翻译"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2c72e88c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from newspaper import Article\n",
    "from datetime import datetime, timezone, timedelta\n",
    "\n",
    "\n",
    "def get_time_info(query):\n",
    "    # 设置为 UTC+8 时区的时间\n",
    "    tz_utc_8 = timezone(timedelta(hours=8))\n",
    "    now = datetime.now(tz_utc_8)  # 获取带时区信息的当前时间\n",
    "\n",
    "    if \"几点\" in query:\n",
    "        return f\"现在是 {now.strftime('%Y-%m-%d %H:%M:%S')}\"\n",
    "    elif \"星期\" in query:\n",
    "        weekdays = [\"一\", \"二\", \"三\", \"四\", \"五\", \"六\", \"日\"]\n",
    "        weekday_cn = weekdays[now.isoweekday() - 1]  # isoweekday(): 1~7（周一到周日）\n",
    "        return f\"今天是星期{weekday_cn}，{now.strftime('%Y-%m-%d')}\"\n",
    "    elif \"几号\" in query or \"日期\" in query:\n",
    "        return f\"今天是 {now.strftime('%Y-%m-%d')}\"\n",
    "    \n",
    "    # 默认提示信息\n",
    "    return \"我可以告诉你时间哦~\"\n",
    "\n",
    "\n",
    "def translate(query):\n",
    "    if \"翻译\" in query:\n",
    "        # 确定翻译方向（中译英/英译中）\n",
    "        target = \"英文\" if \"英文\" in query else \"中文\"\n",
    "        # 清理查询中的翻译指令关键词\n",
    "        content = query.replace(\"翻译\", \"\").replace(\"成英文\", \"\").replace(\"成中文\", \"\")\n",
    "        # 调用模型进行翻译\n",
    "        return qwen_generate(f\"请将以下内容翻译成{target}：{content}\")\n",
    "    return \"翻译请求格式不清晰。\"\n",
    "\n",
    "\n",
    "def summarize_web(url):\n",
    "    try:\n",
    "        # 创建文章对象并设置语言为中文\n",
    "        article = Article(url, language='zh')\n",
    "        # 下载并解析网页内容\n",
    "        article.download()\n",
    "        article.parse()\n",
    "        # 调用Qwen模型进行摘要生成\n",
    "        return qwen_generate(\"请总结以下网页内容：\" + article.text[:2000])\n",
    "    except Exception as e:\n",
    "        return f\"❌ 网页摘要失败：{e}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b87730f",
   "metadata": {},
   "source": [
    "### ✅ Step 9：Agent 路由控制器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0af5bd7d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PDFAgent:\n",
    "    def answer(self, query):\n",
    "        \"\"\"\n",
    "        根据用户输入内容智能分发处理任务\n",
    "        处理逻辑分支:\n",
    "        1. 数学计算 → 调用Qwen模型求解\n",
    "        2. 时间查询 → 调用时间信息模块\n",
    "        3. 翻译请求 → 调用翻译模块\n",
    "        4. 网页链接 → 生成网页摘要\n",
    "        5. 其他问题 → PDF文档检索+Qwen生成答案\n",
    "        \"\"\"\n",
    "        if any(k in query for k in [\"计算\", \"+\", \"-\", \"*\", \"/\"]):\n",
    "            return qwen_generate(f\"请计算：{query}\")\n",
    "        elif any(k in query for k in [\"几点\", \"星期\", \"几号\"]):\n",
    "            return get_time_info(query)\n",
    "        elif \"翻译\" in query:\n",
    "            return translate(query)\n",
    "        elif query.startswith(\"http\"):\n",
    "            return summarize_web(query)\n",
    "        else:\n",
    "            # 1. 从PDF库检索相关内容\n",
    "            retrieved = \"\\n\".join(query_pdf(query))\n",
    "            # 2. 结合上下文生成回答\n",
    "            return qwen_generate(f\"以下是相关资料：{retrieved}\\n请回答：{query}\")\n",
    "\n",
    "agent = PDFAgent()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ecde6c",
   "metadata": {},
   "source": [
    "### ✅ Step 10：测试示例"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284f4be5-f56d-4515-b1b9-0755717484e6",
   "metadata": {},
   "source": [
    "#### RAG知识库背景下的问答示例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a27df70f-2286-4d14-b11c-c934fb65fbe6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "嗯，我现在要回答关于MindSpore的问题。首先，我得仔细阅读提供的资料，了解MindSpore的基本信息和特点。资料中提到，MindSpore是一种全场景覆盖的深度学习计算框架，目标是易开发、高效执行和全场景覆盖。\n",
      "\n",
      "资料里还详细介绍了MindSpore的自动微分机制，使用了源码转换（SCT）和自动微分（AD）技术。这使得构建复杂的模型变得容易，因为可以自动处理流程控制。此外，MindSpore还支持静态编译优化，提升性能。\n",
      "\n",
      "框架由几个主要组件组成：MindExpression（ME）、MindCompiler（MC）、MindData（MD）、MindRE 和MindArmour（MA）。技术贡献包括自动微分、静态编译优化、计算图构建、软硬件协同优化和多场景支持。\n",
      "\n",
      "现在，我需要将这些信息整合成一个简明扼要的解释。首先，概述MindSpore的全场景覆盖和其核心优势，然后详细说明其自动微分机制，接着介绍框架的组件和主要技术贡献。确保用简单易懂的语言，避免专业术语过多，同时涵盖所有关键点。\n",
      "\n",
      "可能的结构是：\n",
      "1. 引言：\n"
     ]
    }
   ],
   "source": [
    "print(agent.answer(\"什么是MindSpore？\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323965e3-3c12-4676-9d88-3a8630b1df10",
   "metadata": {
    "tags": []
   },
   "source": "#### 时间工具调用的示例"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "47a689d6-dd81-498a-80dd-5ce0dc431184",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "现在是 2025-06-05 18:06:15\n"
     ]
    }
   ],
   "source": [
    "print(agent.answer(\"现在几点？\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "99b54226-931c-4b0b-966b-6183aeaca24c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "今天是星期四，2025-06-05\n"
     ]
    }
   ],
   "source": [
    "print(agent.answer(\"今天是星期几？\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c6700c-a76b-4d2c-bf54-5efd37dbc5bd",
   "metadata": {
    "tags": []
   },
   "source": "#### 中英互译工具调用的示例"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b8dea65e-322b-46b9-ab43-c5eb850754c5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "好，用户让我翻译“你好，我是AI助手”成英文。首先，我得确认这句话的结构和意思。中文里“你好”通常翻译成“Hello”，这是比较常见的问候方式。“我是AI助手”这部分需要准确传达出我是人工智能助手的身份，同时保持礼貌和自然。\n",
      "\n",
      "“我是AI助手”中的“我是”可以翻译为“I am”或者“I am your AI assistant”，这样更直接明了。“AI助手”直译是“AI assistant”，但为了更口语化一点，可能可以考虑用“AI helper”或者“AI assistant”，看哪个更合适。\n",
      "\n",
      "接下来，考虑整个句子的流畅性。直接翻译的话是“Hello, I am your AI assistant.”，这样既保留了问候的礼貌，也准确传达了身份。不过，为了更自然，可能可以加上“I am here to help you”来补充说明，让整个翻译更完整。\n",
      "\n",
      "所以，综合起来，翻译成“I am here to help you, my name is AI assistant.”或者“Hello! I'm AI Assistant, how can I help you today?”都可以。前者更直接，后者则更有互动性。\n",
      "\n",
      "用户的需求是翻译这句话，可能用于问候或者自我介绍。考虑到不同的应用场景，提供\n"
     ]
    }
   ],
   "source": [
    "print(agent.answer(\"翻译成英文：你好，我是AI助手\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
