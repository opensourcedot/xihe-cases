{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 网络构建"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "神经网络模型是由神经网络层和Tensor操作构成的，[mindspore.nn](https://www.mindspore.cn/docs/zh-CN/r2.5.0/api_python/mindspore.nn.html)提供了常见神经网络层的实现，在MindSpore中，[Cell](https://www.mindspore.cn/docs/zh-CN/r2.5.0/api_python/nn/mindspore.nn.Cell.html)类是构建所有网络的基类，也是网络的基本单元。一个神经网络模型表示为一个`Cell`，它由不同的子`Cell`构成。使用这样的嵌套结构，可以简单地使用面向对象编程的思维，对神经网络结构进行构建和管理。\n",
    "\n",
    "下面我们将构建一个用于Mnist数据集分类的神经网络模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture captured_output\n",
    "# 实验环境已经预装了mindspore==2.5.0，如需更换mindspore版本，可更改下面 MINDSPORE_VERSION 变量\n",
    "!pip uninstall mindspore -y\n",
    "%env MINDSPORE_VERSION=2.5.0\n",
    "!pip install https://ms-release.obs.cn-north-4.myhuaweicloud.com/${MINDSPORE_VERSION}/MindSpore/unified/x86_64/mindspore-${MINDSPORE_VERSION}-cp39-cp39-linux_x86_64.whl --trusted-host ms-release.obs.cn-north-4.myhuaweicloud.com -i https://pypi.tuna.tsinghua.edu.cn/simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: mindspore\n",
      "Version: 2.5.0\n",
      "Summary: MindSpore is a new open source deep learning training/inference framework that could be used for mobile, edge and cloud scenarios.\n",
      "Home-page: https://www.mindspore.cn\n",
      "Author: The MindSpore Authors\n",
      "Author-email: contact@mindspore.cn\n",
      "License: Apache 2.0\n",
      "Location: /home/nginx/miniconda/envs/jupyter/lib/python3.9/site-packages\n",
      "Requires: asttokens, astunparse, dill, numpy, packaging, pillow, protobuf, psutil, safetensors, scipy\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "# 查看当前 mindspore 版本\n",
    "!pip show mindspore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mindspore\n",
    "from mindspore import nn, ops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 定义模型类\n",
    "\n",
    "当我们定义神经网络时，可以继承`nn.Cell`类，在`__init__`方法中进行子Cell的实例化和状态管理，在`construct`方法中实现Tensor操作。\n",
    "\n",
    "> `construct`意为神经网络（计算图）构建，相关内容详见[使用静态图加速](https://www.mindspore.cn/tutorials/zh-CN/r2.5.0/beginner/accelerate_with_static_graph.html)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Network(nn.Cell):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.dense_relu_sequential = nn.SequentialCell(\n",
    "            nn.Dense(28*28, 512, weight_init=\"normal\", bias_init=\"zeros\"),\n",
    "            nn.ReLU(),\n",
    "            nn.Dense(512, 512, weight_init=\"normal\", bias_init=\"zeros\"),\n",
    "            nn.ReLU(),\n",
    "            nn.Dense(512, 10, weight_init=\"normal\", bias_init=\"zeros\")\n",
    "        )\n",
    "\n",
    "    def construct(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.dense_relu_sequential(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "构建完成后，实例化`Network`对象，并查看其结构。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network<\n",
      "  (flatten): Flatten<>\n",
      "  (dense_relu_sequential): SequentialCell<\n",
      "    (0): Dense<input_channels=784, output_channels=512, has_bias=True>\n",
      "    (1): ReLU<>\n",
      "    (2): Dense<input_channels=512, output_channels=512, has_bias=True>\n",
      "    (3): ReLU<>\n",
      "    (4): Dense<input_channels=512, output_channels=10, has_bias=True>\n",
      "    >\n",
      "  >\n"
     ]
    }
   ],
   "source": [
    "model = Network()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们构造一个输入数据，直接调用模型，可以获得一个二维的Tensor输出，其包含每个类别的原始预测值。\n",
    "\n",
    "> `model.construct()`方法不可直接调用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor(shape=[1, 10], dtype=Float32, value=\n",
       "[[-2.85749696e-03,  1.02917571e-02,  1.32396538e-02 ... -1.44784059e-03, -1.40859755e-02, -1.73981022e-03]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = ops.ones((1, 28, 28), mindspore.float32)\n",
    "logits = model(X)\n",
    "# print logits\n",
    "logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在此基础上，我们通过一个`nn.Softmax`层实例来获得预测概率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: [2]\n"
     ]
    }
   ],
   "source": [
    "pred_probab = nn.Softmax(axis=1)(logits)\n",
    "y_pred = pred_probab.argmax(1)\n",
    "print(f\"Predicted class: {y_pred}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 模型层\n",
    "\n",
    "本节中我们分解上节构造的神经网络模型中的每一层。首先我们构造一个shape为(3, 28, 28)的随机数据（3个28x28的图像），依次通过每一个神经网络层来观察其效果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "input_image = ops.ones((3, 28, 28), mindspore.float32)\n",
    "print(input_image.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### nn.Flatten\n",
    "\n",
    "实例化[nn.Flatten](https://www.mindspore.cn/docs/zh-CN/r2.5.0/api_python/nn/mindspore.nn.Flatten.html)层，将28x28的2D张量转换为784大小的连续数组。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 784)\n"
     ]
    }
   ],
   "source": [
    "flatten = nn.Flatten()\n",
    "flat_image = flatten(input_image)\n",
    "print(flat_image.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### nn.Dense\n",
    "\n",
    "[nn.Dense](https://www.mindspore.cn/docs/zh-CN/r2.5.0/api_python/nn/mindspore.nn.Dense.html)为全连接层，其使用权重和偏差对输入进行线性变换。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 20)\n"
     ]
    }
   ],
   "source": [
    "layer1 = nn.Dense(in_channels=28*28, out_channels=20)\n",
    "hidden1 = layer1(flat_image)\n",
    "print(hidden1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### nn.ReLU\n",
    "\n",
    "[nn.ReLU](https://www.mindspore.cn/docs/zh-CN/r2.5.0/api_python/nn/mindspore.nn.ReLU.html)层给网络中加入非线性的激活函数，帮助神经网络学习各种复杂的特征。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before ReLU: [[ 0.12510496  0.08772035 -0.35773584  0.29153055 -0.9090064  -0.21029618\n",
      "  -0.5256725   0.31484348 -0.5057114   0.06429663 -0.30538613 -0.74424803\n",
      "   0.6760454   0.3550031   0.1573178   0.4786001  -0.45503998  0.1447404\n",
      "  -0.44546226  0.21907322]\n",
      " [ 0.12510496  0.08772035 -0.35773584  0.29153055 -0.9090064  -0.21029618\n",
      "  -0.5256725   0.31484348 -0.5057114   0.06429663 -0.30538613 -0.74424803\n",
      "   0.6760454   0.3550031   0.1573178   0.4786001  -0.45503998  0.1447404\n",
      "  -0.44546226  0.21907322]\n",
      " [ 0.12510496  0.08772035 -0.35773584  0.29153055 -0.9090064  -0.21029618\n",
      "  -0.5256725   0.31484348 -0.5057114   0.06429663 -0.30538613 -0.74424803\n",
      "   0.6760454   0.3550031   0.1573178   0.4786001  -0.45503998  0.1447404\n",
      "  -0.44546226  0.21907322]]\n",
      "\n",
      "\n",
      "After ReLU: [[0.12510496 0.08772035 0.         0.29153055 0.         0.\n",
      "  0.         0.31484348 0.         0.06429663 0.         0.\n",
      "  0.6760454  0.3550031  0.1573178  0.4786001  0.         0.1447404\n",
      "  0.         0.21907322]\n",
      " [0.12510496 0.08772035 0.         0.29153055 0.         0.\n",
      "  0.         0.31484348 0.         0.06429663 0.         0.\n",
      "  0.6760454  0.3550031  0.1573178  0.4786001  0.         0.1447404\n",
      "  0.         0.21907322]\n",
      " [0.12510496 0.08772035 0.         0.29153055 0.         0.\n",
      "  0.         0.31484348 0.         0.06429663 0.         0.\n",
      "  0.6760454  0.3550031  0.1573178  0.4786001  0.         0.1447404\n",
      "  0.         0.21907322]]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Before ReLU: {hidden1}\\n\\n\")\n",
    "hidden1 = nn.ReLU()(hidden1)\n",
    "print(f\"After ReLU: {hidden1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### nn.SequentialCell\n",
    "\n",
    "[nn.SequentialCell](https://www.mindspore.cn/docs/zh-CN/r2.5.0/api_python/nn/mindspore.nn.SequentialCell.html)是一个有序的Cell容器。输入Tensor将按照定义的顺序通过所有Cell。我们可以使用`nn.SequentialCell`来快速组合构造一个神经网络模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 10)\n"
     ]
    }
   ],
   "source": [
    "seq_modules = nn.SequentialCell(\n",
    "    flatten,\n",
    "    layer1,\n",
    "    nn.ReLU(),\n",
    "    nn.Dense(20, 10)\n",
    ")\n",
    "\n",
    "logits = seq_modules(input_image)\n",
    "print(logits.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn.Softmax\n",
    "\n",
    "最后使用[nn.Softmax](https://www.mindspore.cn/docs/zh-CN/r2.5.0/api_python/nn/mindspore.nn.Softmax.html)将神经网络最后一个全连接层返回的logits的值缩放为\\[0, 1\\]，表示每个类别的预测概率。`axis`指定的维度数值和为1。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax = nn.Softmax(axis=1)\n",
    "pred_probab = softmax(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 模型参数\n",
    "\n",
    "网络内部神经网络层具有权重参数和偏置参数（如`nn.Dense`），这些参数会在训练过程中不断进行优化，可通过 `model.parameters_and_names()` 来获取参数名及对应的参数详情。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model structure: Network<\n",
      "  (flatten): Flatten<>\n",
      "  (dense_relu_sequential): SequentialCell<\n",
      "    (0): Dense<input_channels=784, output_channels=512, has_bias=True>\n",
      "    (1): ReLU<>\n",
      "    (2): Dense<input_channels=512, output_channels=512, has_bias=True>\n",
      "    (3): ReLU<>\n",
      "    (4): Dense<input_channels=512, output_channels=10, has_bias=True>\n",
      "    >\n",
      "  >\n",
      "\n",
      "\n",
      "Layer: dense_relu_sequential.0.weight\n",
      "Size: (512, 784)\n",
      "Values : [[ 0.00446854 -0.01434032  0.01026828 ... -0.00413149 -0.00703896\n",
      "   0.00033615]\n",
      " [ 0.00086491 -0.01650446  0.00639932 ...  0.00399117 -0.0067675\n",
      "   0.01557065]] \n",
      "\n",
      "Layer: dense_relu_sequential.0.bias\n",
      "Size: (512,)\n",
      "Values : [0. 0.] \n",
      "\n",
      "Layer: dense_relu_sequential.2.weight\n",
      "Size: (512, 512)\n",
      "Values : [[ 0.00561533  0.00337315 -0.01650148 ...  0.00706137 -0.00842737\n",
      "   0.00351488]\n",
      " [-0.00725421 -0.0010514  -0.0254494  ... -0.00852835 -0.00941908\n",
      "  -0.0025794 ]] \n",
      "\n",
      "Layer: dense_relu_sequential.2.bias\n",
      "Size: (512,)\n",
      "Values : [0. 0.] \n",
      "\n",
      "Layer: dense_relu_sequential.4.weight\n",
      "Size: (10, 512)\n",
      "Values : [[ 0.0027331   0.00096246  0.00104672 ...  0.00023582 -0.00728893\n",
      "  -0.00789147]\n",
      " [ 0.00201431  0.00723096  0.01245865 ...  0.01305133 -0.00046122\n",
      "  -0.00228   ]] \n",
      "\n",
      "Layer: dense_relu_sequential.4.bias\n",
      "Size: (10,)\n",
      "Values : [0. 0.] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model structure: {model}\\n\\n\")\n",
    "\n",
    "for name, param in model.parameters_and_names():\n",
    "    print(f\"Layer: {name}\\nSize: {param.shape}\\nValues : {param[:2]} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "更多内置神经网络层详见[mindspore.nn API](https://www.mindspore.cn/docs/zh-CN/r2.5.0/api_python/mindspore.nn.html)。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
