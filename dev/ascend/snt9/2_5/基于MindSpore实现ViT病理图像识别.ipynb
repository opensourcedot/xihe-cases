{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b69f2b5-5c6f-4c3d-91b8-342f707fbcf9",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 背景介绍\n",
    "> \n",
    "+ 在本项目中，基于MindSpore框架使用 MedMNIST 提供的 PathMNIST 数据集，利用预训练的 Vision Transformer（ViT）模型对结直肠组织图像进行多分类任务；\n",
    "+ PathMNIST 为 MedMNIST v2 中的一个子集，专注于结直肠癌组织切片的多分类任务，共包含约 100,000 张 28×28 彩色图像，涵盖 9 类组织类型，包括正常粘膜、癌相关间质、淋巴细胞等。  PathMNIST 数据集经过统一尺寸与预处理，便于快速原型开发与模型评估；  \n",
    "+ 结直肠癌是全球发病率和死亡率较高的恶性肿瘤之一，早期病理筛查与分类对提高诊断准确性和治疗效果至关重要。利用自动化图像识别技术，可减轻病理专家工作量并提升诊断一致性。      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450e9caa-9939-4f41-9f6c-dd9ba45e6331",
   "metadata": {
    "tags": []
   },
   "source": [
    "## ViT模型简介\n",
    "Vision Transformer（ViT）负责提取图像特征，将输入图像编码为视觉表征，其核心思想——**用注意力重新定义视觉信息的表达**，仅处理图像输入，不参与语言理解或生成。Vision Transformer（ViT）是Transformer架构从自然语言处理（NLP）领域向计算机视觉（CV）领域拓展的里程碑式创新。它摒弃了传统卷积神经网络（CNN）依赖局部卷积核的设计，**将图像视为“序列化的文本”**，通过全局注意力机制直接捕捉图像中远距离像素间的依赖关系。核心技术与实现方式：\n",
    ">- 输入图像被划分为多个 Patch（如 3x3的块），通过线性投影得到 Patch Embedding。\n",
    ">- 加入位置编码（Positional Encoding）后，输入到多层 Transformer Encoder 中，提取全局上下文特征。\n",
    ">- 输出为图像的特征序列（如 [CLS] token + Patch Embeddings）。\n",
    "\n",
    "![架构图](https://pic4.zhimg.com/v2-b6861b011c966bdeeb0a6236a38b3ce5_r.jpg)\n",
    "\n",
    "优势是：相比 CNN，ViT **能捕捉长距离依赖关系，适合处理复杂视觉场景**。可复用 ImageNet 等大规模数据集上的预训练权重，提升泛化能力。ViT的诞生标志着CV领域从“局部卷积”到“全局注意力”的范式转变，后续衍生出DeiT（数据高效）、Swin Transformer（窗口化注意力）、BEiT（掩码预训练）等变体，推动视觉任务（分类、检测、分割）迈向更高效、灵活的模型架构。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e409768-1100-4d76-a756-b8172cead8c6",
   "metadata": {},
   "source": [
    "## 环境配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b61724-09ae-4d55-907f-9fb955a6d60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture captured_output\n",
    "# 实验环境已经预装了mindspore==2.5.0，如需更换mindspore版本，可更改下面 MINDSPORE_VERSION 变量\n",
    "!pip uninstall mindspore -y\n",
    "%env MINDSPORE_VERSION=2.5.0\n",
    "!pip install https://ms-release.obs.cn-north-4.myhuaweicloud.com/${MINDSPORE_VERSION}/MindSpore/unified/aarch64/mindspore-${MINDSPORE_VERSION}-cp39-cp39-linux_aarch64.whl --trusted-host ms-release.obs.cn-north-4.myhuaweicloud.com -i https://pypi.tuna.tsinghua.edu.cn/simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5eeb9f1-4e69-42a7-be65-9641f3653403",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: mindspore\n",
      "Version: 2.5.0\n",
      "Summary: MindSpore is a new open source deep learning training/inference framework that could be used for mobile, edge and cloud scenarios.\n",
      "Home-page: https://www.mindspore.cn\n",
      "Author: The MindSpore Authors\n",
      "Author-email: contact@mindspore.cn\n",
      "License: Apache 2.0\n",
      "Location: /home/mindspore/miniconda/envs/jupyter/lib/python3.9/site-packages\n",
      "Requires: asttokens, astunparse, dill, numpy, packaging, pillow, protobuf, psutil, safetensors, scipy\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "# 查看当前 mindspore 版本\n",
    "!pip show mindspore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b442a9",
   "metadata": {},
   "source": [
    "通过环境变量配置日志级别来控制 MindSpore 的日志详细程度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf567408",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Set log level: 4(EXCEPTION), 3(ERROR), 2(WARNING), 1(INFO), 0(DEBUG)\n",
    "os.environ['GLOG_v'] = '3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a663d2ce-a95e-489d-947a-f2d7ba4a85a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture captured_output\n",
    "!pip install medmnist\n",
    "!export HF_ENDPOINT=https://hf-mirror.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d6ee8c-b795-4fc3-a4d2-b12c08806e62",
   "metadata": {},
   "source": [
    "## 数据集加载及预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "855d24e9-1591-49fe-b8f0-68dc75c31be9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from medmnist import INFO, PathMNIST\n",
    "import mindspore.dataset.vision as vision\n",
    "from mindspore.dataset import GeneratorDataset\n",
    "\n",
    "\n",
    "# 指定数据集\n",
    "data_flag = 'pathmnist'\n",
    "download_root = './data'\n",
    "os.makedirs(download_root, exist_ok=True) \n",
    "# 从 INFO 中读取所有可用字段\n",
    "info = INFO[data_flag]\n",
    "\n",
    "# 直接根据标签列表计算类别数\n",
    "labels = info['label']\n",
    "num_classes = len(labels)\n",
    "n_channels = info['n_channels']\n",
    "\n",
    "# 下载并加载数据\n",
    "train_dataset = PathMNIST(root=download_root, split='train', download=True)\n",
    "val_dataset = PathMNIST(root=download_root, split='val',   download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ac6a6e6-fc1a-4efb-a59e-41a8973afab2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from mindspore import dataset as ds\n",
    "\n",
    "# 数据预处理与 DataLoader 创建函数\n",
    "def create_dataset(dataset, batch_size=64, shuffle=True):\n",
    "    transform = [\n",
    "        vision.Resize((224, 224)),\n",
    "        vision.ToTensor(),\n",
    "    ]\n",
    "    ds_gen = ds.GeneratorDataset(dataset, column_names=['image','label'], shuffle=shuffle)\n",
    "    ds_gen = ds_gen.map(input_columns=['image'], operations=transform)\n",
    "    ds_gen = ds_gen.map(input_columns=['label'], operations=lambda x: x.squeeze().astype('int32'))\n",
    "    ds_gen = ds_gen.batch(batch_size)\n",
    "    return ds_gen\n",
    "\n",
    "train_ds = create_dataset(train_dataset, batch_size=64, shuffle=True)\n",
    "val_ds   = create_dataset(val_dataset,   batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823d09c5-fac1-4f36-90bb-3a8a002ce4e6",
   "metadata": {},
   "source": [
    "## 网络构建\n",
    "ViT 通过将图像划分为固定大小的“patch”并在 Transformer 编码器上进行自注意力计算，展现出在各类视觉任务上媲美或超越传统 CNN 的性能；采用 MindVision 提供的 vit_b_16 模型，该模型在 ImageNet-21k 上预训练，并在 ImageNet-1k 上微调后提供良好泛化性能；将原有的 1000 类分类头替换为适应 PathMNIST 的 9 类输出层，保留主干 Transformer 权重，以加速下游任务训练。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1d5567-477b-40af-a2f4-4f791d205401",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import mindspore as ms\n",
    "from mindspore import nn, context\n",
    "from mindspore.train.callback import LossMonitor, ModelCheckpoint, CheckpointConfig\n",
    "from mindvision.classification.models import vit_b_16\n",
    "from mindspore.train import Model\n",
    "from mindspore.train.metrics import Accuracy\n",
    "\n",
    "\n",
    "# 加载预训练 ViT 并替换分类头\n",
    "net = vit_b_16(pretrained=True, num_classes=1000, image_size=224)\n",
    "net.head = nn.Dense(in_channels=768, out_channels=num_classes)  # 9 类\n",
    "\n",
    "# 定义损失函数与优化器\n",
    "loss_fn = nn.SoftmaxCrossEntropyWithLogits(sparse=True, reduction='mean')\n",
    "optimizer = nn.Adam(net.trainable_params(), learning_rate=1e-4)\n",
    "\n",
    "# 初始化 Accuracy 指标\n",
    "accuracy_metric = Accuracy()\n",
    "\n",
    "# 定义 Model，确保 metrics 为字典格式\n",
    "model = Model(net, loss_fn=loss_fn, optimizer=optimizer, metrics={\"Accuracy\": accuracy_metric})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7d94b7-292a-410c-a3a6-0d3c247a6698",
   "metadata": {},
   "source": [
    "## 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba50edca-74a1-4fbe-8174-09679e5e768a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-14 02:42:10] Evaluating on validation set...\n",
      "Start training for 5 epochs, dataset size: 1407 batches\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".epoch: 1 step: 50, loss is 2.2472798824310303\n",
      "epoch: 1 step: 100, loss is 2.205869197845459\n",
      "epoch: 1 step: 150, loss is 2.380136013031006\n",
      "epoch: 1 step: 200, loss is 1.8169846534729004\n",
      "epoch: 1 step: 250, loss is 1.7921384572982788\n",
      "epoch: 1 step: 300, loss is 1.3815851211547852\n",
      "epoch: 1 step: 350, loss is 1.4965860843658447\n",
      "epoch: 1 step: 400, loss is 1.318996548652649\n",
      "epoch: 1 step: 450, loss is 1.1503033638000488\n",
      "epoch: 1 step: 500, loss is 1.0637834072113037\n",
      "epoch: 1 step: 550, loss is 1.2114602327346802\n",
      "epoch: 1 step: 600, loss is 1.1093289852142334\n",
      "epoch: 1 step: 650, loss is 1.1249001026153564\n",
      "epoch: 1 step: 700, loss is 1.189340591430664\n",
      "epoch: 1 step: 750, loss is 0.9763270616531372\n",
      "epoch: 1 step: 800, loss is 0.9399876594543457\n",
      "epoch: 1 step: 850, loss is 0.7920712232589722\n",
      "epoch: 1 step: 900, loss is 0.7658846378326416\n",
      "epoch: 1 step: 950, loss is 0.9221714735031128\n",
      "epoch: 1 step: 1000, loss is 0.8943929672241211\n",
      "epoch: 1 step: 1050, loss is 1.0630111694335938\n",
      "epoch: 1 step: 1100, loss is 0.9335418939590454\n",
      "epoch: 1 step: 1150, loss is 1.439079999923706\n",
      "epoch: 1 step: 1200, loss is 1.055544376373291\n",
      "epoch: 1 step: 1250, loss is 1.233201503753662\n",
      "epoch: 1 step: 1300, loss is 0.9616548418998718\n",
      "epoch: 1 step: 1350, loss is 1.0072898864746094\n",
      "epoch: 1 step: 1400, loss is 0.7312507629394531\n",
      "epoch: 2 step: 43, loss is 0.784365177154541\n",
      "epoch: 2 step: 93, loss is 0.8988298773765564\n",
      "epoch: 2 step: 143, loss is 0.7859616279602051\n",
      "epoch: 2 step: 193, loss is 1.06912362575531\n",
      "epoch: 2 step: 243, loss is 0.9633661508560181\n",
      "epoch: 2 step: 293, loss is 1.0448079109191895\n",
      "epoch: 2 step: 343, loss is 0.8862581253051758\n",
      "epoch: 2 step: 393, loss is 0.8124474287033081\n",
      "epoch: 2 step: 443, loss is 0.6983298063278198\n",
      "epoch: 2 step: 493, loss is 0.678682804107666\n",
      "epoch: 2 step: 543, loss is 0.8405871391296387\n",
      "epoch: 2 step: 593, loss is 1.035875678062439\n",
      "epoch: 2 step: 643, loss is 1.3095712661743164\n",
      "epoch: 2 step: 693, loss is 0.554206132888794\n",
      "epoch: 2 step: 743, loss is 0.5986025333404541\n",
      "epoch: 2 step: 793, loss is 0.8859332799911499\n",
      "epoch: 2 step: 843, loss is 0.6067121028900146\n",
      "epoch: 2 step: 893, loss is 0.8442614078521729\n",
      "epoch: 2 step: 943, loss is 0.5622591376304626\n",
      "epoch: 2 step: 993, loss is 0.9193767309188843\n",
      "epoch: 2 step: 1043, loss is 0.7834809422492981\n",
      "epoch: 2 step: 1093, loss is 0.6135741472244263\n",
      "epoch: 2 step: 1143, loss is 0.7809973955154419\n",
      "epoch: 2 step: 1193, loss is 0.6615335941314697\n",
      "epoch: 2 step: 1243, loss is 0.7350854277610779\n",
      "epoch: 2 step: 1293, loss is 0.5447252988815308\n",
      "epoch: 2 step: 1343, loss is 0.6243122816085815\n",
      "epoch: 2 step: 1393, loss is 0.6710508465766907\n",
      "epoch: 3 step: 36, loss is 0.4081033766269684\n",
      "epoch: 3 step: 86, loss is 0.9941943883895874\n",
      "epoch: 3 step: 136, loss is 0.6606602668762207\n",
      "epoch: 3 step: 186, loss is 0.7246103286743164\n",
      "epoch: 3 step: 236, loss is 0.6864809989929199\n",
      "epoch: 3 step: 286, loss is 0.8686243295669556\n",
      "epoch: 3 step: 336, loss is 0.6450804471969604\n",
      "epoch: 3 step: 386, loss is 0.681995153427124\n",
      "epoch: 3 step: 436, loss is 0.5821930170059204\n",
      "epoch: 3 step: 486, loss is 0.6443427801132202\n",
      "epoch: 3 step: 536, loss is 0.48640185594558716\n",
      "epoch: 3 step: 586, loss is 0.6519737243652344\n",
      "epoch: 3 step: 636, loss is 0.49280035495758057\n",
      "epoch: 3 step: 686, loss is 0.5610111355781555\n",
      "epoch: 3 step: 736, loss is 0.4237068295478821\n",
      "epoch: 3 step: 786, loss is 0.5343749523162842\n",
      "epoch: 3 step: 836, loss is 0.6903446912765503\n",
      "epoch: 3 step: 886, loss is 0.5658241510391235\n",
      "epoch: 3 step: 936, loss is 0.4901997148990631\n",
      "epoch: 3 step: 986, loss is 0.5646941661834717\n",
      "epoch: 3 step: 1036, loss is 0.7793715000152588\n",
      "epoch: 3 step: 1086, loss is 0.603563666343689\n",
      "epoch: 3 step: 1136, loss is 0.746092677116394\n",
      "epoch: 3 step: 1186, loss is 0.4593072831630707\n",
      "epoch: 3 step: 1236, loss is 0.6799904108047485\n",
      "epoch: 3 step: 1286, loss is 0.5561709403991699\n",
      "epoch: 3 step: 1336, loss is 0.48030799627304077\n",
      "epoch: 3 step: 1386, loss is 0.7677222490310669\n",
      "epoch: 4 step: 29, loss is 0.7060439586639404\n",
      "epoch: 4 step: 79, loss is 0.46739599108695984\n",
      "epoch: 4 step: 129, loss is 0.7019684314727783\n",
      "epoch: 4 step: 179, loss is 0.5416550040245056\n",
      "epoch: 4 step: 229, loss is 0.346041738986969\n",
      "epoch: 4 step: 279, loss is 0.626590371131897\n",
      "epoch: 4 step: 329, loss is 0.4444848895072937\n",
      "epoch: 4 step: 379, loss is 0.5208226442337036\n",
      "epoch: 4 step: 429, loss is 0.3987398147583008\n",
      "epoch: 4 step: 479, loss is 0.41211050748825073\n",
      "epoch: 4 step: 529, loss is 0.44395971298217773\n",
      "epoch: 4 step: 579, loss is 0.5950093269348145\n",
      "epoch: 4 step: 629, loss is 0.39946848154067993\n",
      "epoch: 4 step: 679, loss is 0.43145930767059326\n",
      "epoch: 4 step: 729, loss is 0.38877445459365845\n",
      "epoch: 4 step: 779, loss is 0.32453542947769165\n",
      "epoch: 4 step: 829, loss is 0.3673306107521057\n",
      "epoch: 4 step: 879, loss is 0.4985491931438446\n",
      "epoch: 4 step: 929, loss is 0.4623561501502991\n",
      "epoch: 4 step: 979, loss is 0.48968517780303955\n",
      "epoch: 4 step: 1029, loss is 0.46413642168045044\n",
      "epoch: 4 step: 1079, loss is 0.17528748512268066\n",
      "epoch: 4 step: 1129, loss is 0.45291510224342346\n",
      "epoch: 4 step: 1179, loss is 0.3475700616836548\n",
      "epoch: 4 step: 1229, loss is 0.47113803029060364\n",
      "epoch: 4 step: 1279, loss is 0.5274629592895508\n",
      "epoch: 4 step: 1329, loss is 0.6214989423751831\n",
      "epoch: 4 step: 1379, loss is 0.49477890133857727\n",
      "epoch: 5 step: 22, loss is 0.30412185192108154\n",
      "epoch: 5 step: 72, loss is 0.6273037195205688\n",
      "epoch: 5 step: 122, loss is 0.4417189955711365\n",
      "epoch: 5 step: 172, loss is 0.2364601194858551\n",
      "epoch: 5 step: 222, loss is 0.2876967787742615\n",
      "epoch: 5 step: 272, loss is 0.5529639720916748\n",
      "epoch: 5 step: 322, loss is 0.17262661457061768\n",
      "epoch: 5 step: 372, loss is 0.39950698614120483\n",
      "epoch: 5 step: 422, loss is 0.348679780960083\n",
      "epoch: 5 step: 472, loss is 0.33328738808631897\n",
      "epoch: 5 step: 522, loss is 0.3103867173194885\n",
      "epoch: 5 step: 572, loss is 0.3112157881259918\n",
      "epoch: 5 step: 622, loss is 0.26289474964141846\n",
      "epoch: 5 step: 672, loss is 0.23463338613510132\n",
      "epoch: 5 step: 722, loss is 0.3460732102394104\n",
      "epoch: 5 step: 772, loss is 0.4614480137825012\n",
      "epoch: 5 step: 822, loss is 0.5048486590385437\n",
      "epoch: 5 step: 872, loss is 0.30573028326034546\n",
      "epoch: 5 step: 922, loss is 0.21578165888786316\n",
      "epoch: 5 step: 972, loss is 0.48813527822494507\n",
      "epoch: 5 step: 1022, loss is 0.21820330619812012\n",
      "epoch: 5 step: 1072, loss is 0.2280353158712387\n",
      "epoch: 5 step: 1122, loss is 0.3255307674407959\n",
      "epoch: 5 step: 1172, loss is 0.486792653799057\n",
      "epoch: 5 step: 1222, loss is 0.3173666298389435\n",
      "epoch: 5 step: 1272, loss is 0.33936938643455505\n",
      "epoch: 5 step: 1322, loss is 0.399868905544281\n",
      "epoch: 5 step: 1372, loss is 0.3915732502937317\n",
      "Train time: 2513890.38 ms\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# 开始评估\n",
    "train_start = time.perf_counter()\n",
    "print(f\"[{datetime.now():%Y-%m-%d %H:%M:%S}] Evaluating on validation set...\")\n",
    "\n",
    "# 每 100 步保存一次，最多保留 3 个检查点\n",
    "ckpt_config = CheckpointConfig(save_checkpoint_steps=100, keep_checkpoint_max=3)\n",
    "ckpt_cb = ModelCheckpoint(prefix=\"vit_pathmnist\",\n",
    "                          directory=\"./checkpoints\",\n",
    "                          config=ckpt_config)\n",
    "\n",
    "# 开始训练\n",
    "epoch_num = 5\n",
    "print(f\"Start training for {epoch_num} epochs, dataset size: {train_ds.get_dataset_size()} batches\")\n",
    "model.train(epoch_num,\n",
    "            train_dataset=train_ds,\n",
    "            callbacks=[LossMonitor(per_print_times=50), ckpt_cb],\n",
    "            dataset_sink_mode=False)\n",
    "\n",
    "# 结束评估\n",
    "train_end = time.perf_counter()\n",
    "elapsed_ms = (train_end - train_start) * 1000\n",
    "\n",
    "print(f\"Train time: {elapsed_ms:.2f} ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24dd1860-d699-4546-8e57-0078b38ecb9a",
   "metadata": {},
   "source": [
    "## 模型评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fad86f6e-3356-4b83-83d8-849563bd5684",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-14 03:27:14] Evaluating on validation set...\n",
      "Accuracy: 0.8784\n",
      "Evaluation time: 19712.50 ms\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# 开始评估\n",
    "eval_start = time.perf_counter()\n",
    "print(f\"[{datetime.now():%Y-%m-%d %H:%M:%S}] Evaluating on validation set...\")\n",
    "\n",
    "# 评估\n",
    "metrics = model.eval(val_ds, dataset_sink_mode=False)\n",
    "\n",
    "# 结束评估\n",
    "eval_end = time.perf_counter()\n",
    "elapsed_ms = (eval_end - eval_start) * 1000\n",
    "\n",
    "# 打印各项指标\n",
    "for name, value in metrics.items():\n",
    "    print(f\"{name}: {value:.4f}\")\n",
    "\n",
    "print(f\"Evaluation time: {elapsed_ms:.2f} ms\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
